{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word prediction in historical linguistics\n",
    "This is a Jupyter notebook and Python library to demonstrate the use of word prediction using deep learning as an aid in historical linguistics. This notebook is based on [master thesis work by Peter Dekker](http://peterdekker.eu/projects/#mscthesis). The results yielded by this demonstrational notebook may differ somewhat from the results in the thesis.\n",
    "\n",
    "Any questions or problems?\n",
    " * [Contact me](https://peterdekker.eu/#contact)\n",
    " * [File a bug report](https://github.com/peterdekker/prediction-histling/issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application and data loading\n",
    "Run this cell before running other cells. Execution of this cell takes a long time the first time, since automatic cognate judgments (using external LexStat algorithm) are inferred for the whole data set. These cognate judgments are used to perform evaluation of the word prediction algorithm.\n",
    "\n",
    "The following settings can be changed by the user:\n",
    " * Set languages by alternatively setting one of these two variables:\n",
    "   * `languages`: Give a list of languages (language codes from [NorthEuraLex](http://northeuralex.org/languages)) to take under consideration.\n",
    "   * `lang_family`: `slav` or `ger`. If `lang_family` has been set (not `None`), `languages` will be disregarded.\n",
    " * `cognate_detection`: `False` by default, which allows multiple forms per concept, which leads to a larger data set. If you would like to be able to perform cognate detection based on prediction results (a later step in this notebook), set this variable to `True`, which restricts the data set to one form per concept.\n",
    " * `input_encoding`: the procedure used to encode phonemes into input vectors. `embedding` by default. Can be set to `character` (one-hot), `phonetic` or `embedding`.\n",
    " * `export_weights`: `False` by default. Set to `True` to be able to export and visualize the weights of the network. Do this only for diagnostic/visualization purposes, because the batch size will be set to 1, which is not optimal.\n",
    " * `view_embedding_ipa`: `True` by default. Internally, the ASJP character set is used to predict words. To view the embedding encoding visualization as IPA, set this to True. If False, embedding encoding visualization will be shown in ASJP. Note: this option is purely aesthetical.\n",
    "\n",
    "\n",
    "Other, more advanced, options can be set in the file `util/config.py`, the notebook should then be reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T15:07:51.388051Z",
     "start_time": "2019-08-08T15:07:14.853998Z"
    }
   },
   "outputs": [],
   "source": [
    "from util import init\n",
    "from dataset import data\n",
    "from util.config import config\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "lang_family_dict = {\n",
    "\"slav\": [\"ces\", \"bul\", \"rus\", \"bel\", \"ukr\", \"pol\", \"slk\", \"slv\", \"hrv\"],\n",
    "\"ger\": [\"swe\", \"isl\", \"eng\", \"nld\", \"deu\", \"dan\", \"nor\"]\n",
    "}\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# VARIABLES TO BE SET BY USER\n",
    "languages = [\"nld\",\"deu\"]\n",
    "lang_family = None\n",
    "cognate_detection = False\n",
    "config[\"export_weights\"] = False\n",
    "config[\"input_encoding\"] = \"embedding\"\n",
    "config[\"view_embedding_ipa\"] = True\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "if config[\"export_weights\"]:\n",
    "    config[\"batch_size\"]=1\n",
    "\n",
    "if lang_family:\n",
    "    languages = lang_family_dict[lang_family]\n",
    "\n",
    "    \n",
    "options, distances_path, baselines_path = init.initialize_program(cognate_detection, config=config)\n",
    "(results_path, output_path_cognates_train, output_path_cognates_valtest,\n",
    "context_vectors_path, subs_sp_path, subs_st_path, lang_pairs, train, val, test, max_len, \n",
    "conversion_key, voc_size, feature_matrix_phon) = data.load_data(train_corpus=\"northeuralex\",\n",
    "                                                               valtest_corpus=\"northeuralex\",\n",
    "                                                               languages=languages,  \n",
    "                                                               input_type=\"asjp\", \n",
    "                                                               options=options,\n",
    "                                                               cognate_detection=False,\n",
    "                                                               config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize encoding\n",
    "Visualize the representation of phonemes in the embedding encoding, as PCA and as hierarchically clustered tree. Compare them to the phonetic feature matrix from Brown (2008)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.160590Z",
     "start_time": "2019-08-08T12:37:17.651Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import data\n",
    "from visualize import visualize\n",
    "from tree import cluster\n",
    "from util.asjp2ipa import asjp_to_ipa\n",
    "\n",
    "tree_style = config[\"ete_tree_style\"]\n",
    "\n",
    "print(\"Phonetic matrix from Brown (2008):\")\n",
    "# Perform PCA on phonetic feature matrix from Brown (2008)\n",
    "phon_matrix_red, phon_phonemes = visualize.dim_reduction(feature_matrix_phon)\n",
    "# If enabled, convert ASJP phonemes to IPA for plot\n",
    "if config[\"view_embedding_ipa\"]:\n",
    "    phon_phonemes = [asjp_to_ipa(p) for p in phon_phonemes]\n",
    "# Visualize phonetic feature PCA using plot\n",
    "visualize.visualize_encoding(phon_matrix_red, phon_phonemes, \"phonetic-pca\", config)\n",
    "# Hierarchically cluster distances between phonemes in phonetic feature matrix\n",
    "tree = cluster.cluster_phonemes_encoding(feature_matrix_phon, phon_phonemes, \"phonetic\", config)\n",
    "\n",
    "display(tree.render(\"%%inline\", tree_style=tree_style))\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    lang_a = lang_pair[0]\n",
    "    print(f\"Embedding for {lang_a}:\")\n",
    "    # Create embedding for every first language in language pair\n",
    "    emb_matrix = data.create_embedding(lang_a, [output_path_cognates_train, output_path_cognates_valtest], config)\n",
    "    # Perform PCA on embedding matrix\n",
    "    emb_matrix_red, emb_phonemes = visualize.dim_reduction(emb_matrix)\n",
    "    # If enabled, convert ASJP phonemes to IPA for plot\n",
    "    if config[\"view_embedding_ipa\"]:\n",
    "        emb_phonemes = [asjp_to_ipa(p) for p in emb_phonemes]\n",
    "    # Visualize embedding PCA using plot\n",
    "    visualize.visualize_encoding(emb_matrix_red, emb_phonemes, f\"embedding-{lang_pair[0]}-pca\", config)\n",
    "    \n",
    "    # Hierarchically cluster distances between phonemes in embedding matrix\n",
    "    tree = cluster.cluster_phonemes_encoding(emb_matrix, emb_phonemes, f\"embedding-{lang_pair[0]}\", config)\n",
    "    display(tree.render(\"%%inline\", tree_style=tree_style))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close-up: constrast occurrences of phonemes in data\n",
    "Interesting patterns in the phoneme encoding visualizations, can be looked up in the data. In the Dutch embedding encoding, we saw that *t* and *d*, closely related phonemes, are quite remote in the embedding space. How do the words with *t* and *d*, on which the embedding encoding is based, look in Dutch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lang = \"nld\"\n",
    "# Read in TSV file with data\n",
    "df = pd.read_csv(output_path_cognates_train, sep=\"\\t\", engine=\"python\", skipfooter=3, index_col=False)\n",
    "df_lang = df[df[\"DOCULECT\"] == lang]\n",
    "\n",
    "for phoneme in [\"d\", \"t\"]:\n",
    "    print(phoneme)\n",
    "    words_with_phoneme = df_lang[df_lang[\"ASJP\"].str.contains(phoneme)]\n",
    "    total = len(words_with_phoneme)\n",
    "    print(f\"Total number of occurrences: {total}\")\n",
    "    \n",
    "    # Compute locations of phonemes in word\n",
    "    locations = df_lang[\"ASJP\"].str.find(phoneme)\n",
    "    locations = locations[locations != -1]\n",
    "    # Compute relative frequencies\n",
    "    locations_relfreq = locations.value_counts(normalize=True)\n",
    "    print(\"Relative frequencies of locations:\")\n",
    "    print(locations_relfreq)\n",
    "    # Look up words with most frequent location\n",
    "    most_freq_location = int(locations.mode())\n",
    "    words_in_most_freq_loc = words_with_phoneme[words_with_phoneme[\"ASJP\"].str.find(phoneme) == most_freq_location]\n",
    "    print(f\"Words with {phoneme} where {phoneme} has most frequent location in word ({most_freq_location}):\")\n",
    "    print(words_in_most_freq_loc)\n",
    "    print(\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Show number of cognates in training data\n",
    "Show the number of cognate word pairs per language pair in the training data, and calculate cliques of languages with a minimum of 100 shared cognates. These cliques can later be used, to have a group of languages with a large shared number of cognates, to perform prediction on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.156995Z",
     "start_time": "2019-08-08T12:36:42.898Z"
    }
   },
   "outputs": [],
   "source": [
    "cog_per_lang, cliques = data.compute_n_cognates(lang_pairs, output_path_cognates_train, langs=languages, cognates_threshold=100, config=config)\n",
    "print(\"Cognates per language: \")\n",
    "print(cog_per_lang)\n",
    "print(\"Cliques: \")\n",
    "for c in cliques:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise word prediction\n",
    "Choose a model to run to perform word prediction: structured perceptron or RNN encoder-decoder. Structured perceptron has the shortest run time. The results of the model run latest for a language pair are saved, and can be used for subsequent applications in the notebook (e.g. phylogenetic tree reconstruction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word prediction using structured perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.158867Z",
     "start_time": "2019-08-08T12:37:12.451Z"
    }
   },
   "outputs": [],
   "source": [
    "from prediction import prediction\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    lang_a, lang_b = lang_pair\n",
    "    print(\"Performing structured perceptron word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "    prediction.word_prediction_seq(lang_a, lang_b, train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], results_path[lang_pair], distances_path + \".txt\", config)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word prediction using encoder-decoder\n",
    "For enabling GPU support, refer to the [Lasagne documentation](https://lasagne.readthedocs.io/en/latest/user/installation.html#gpu-support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.157919Z",
     "start_time": "2019-08-08T12:37:07.979Z"
    }
   },
   "outputs": [],
   "source": [
    "from prediction import prediction\n",
    "for lang_pair in lang_pairs:\n",
    "    lang_a,lang_b = lang_pair\n",
    "    print(\"Performing RNN word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "    prediction.word_prediction_rnn(lang_a, lang_b, (max_len[lang_a], max_len[lang_b]), train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], (voc_size[lang_a],voc_size[lang_b]), results_path[lang_pair], distances_path + \".txt\", context_vectors_path[lang_pair] + \".p\", config[\"output_encoding\"], config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.159715Z",
     "start_time": "2019-08-08T12:37:14.818Z"
    }
   },
   "outputs": [],
   "source": [
    "from models import baseline\n",
    "\n",
    "# Only ASJP\n",
    "for lang_pair in lang_pairs:\n",
    "    lang_a,lang_b = lang_pair\n",
    "    conv = conversion_key[lang_pair]\n",
    "    sounds = (list(conv[0].values()), list(conv[1].values()))\n",
    "    training_frame = train[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "    testing_frame = test[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "    baseline.compute_baseline(lang_a, lang_b, sounds, training_frame, testing_frame, baselines_path + \".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify sound correspondences\n",
    "##### Based on output substitutions\n",
    "This shows the substitutions table in the notebook, and outputs it as LaTeX table to `RESULTS_DIR/subs-LANG1-LANG2.tex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.161412Z",
     "start_time": "2019-08-08T12:37:20.235Z"
    }
   },
   "outputs": [],
   "source": [
    "from visualize import visualize\n",
    "for lang_pair in lang_pairs:\n",
    "    lang_a, lang_b = lang_pair\n",
    "    visualize.show_output_substitutions(results_path[lang_pair], subs_st_path[lang_pair], subs_sp_path[lang_pair], lang_a, lang_b, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on context vector weights (only when weight exported activated)\n",
    "This works when word prediction with encoder-decoder has been run, and weight export has been activated at the top of this notebook. Executing this command shows the distances between input, output and context vectors. Furthermore, it generates PCA plots of the input, output and context vectors and generates LaTeX files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from visualize import visualize\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    visualize.visualize_weights(context_vectors_path[lang_pair], lang_pair, config[\"input_encoding\"], config[\"output_encoding\"], config[\"results_dir\"], sample=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phylogenetic tree reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering based on word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.162205Z",
     "start_time": "2019-08-08T12:37:22.524Z"
    }
   },
   "outputs": [],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Cluster based on word prediction distances\n",
    "trees = cluster.cluster_languages(lang_pairs, distances_path, output_path=distances_path, config=config)\n",
    "\n",
    "# Show trees in notebook\n",
    "for tree in trees:\n",
    "    print(tree)\n",
    "    display(tree.render(\"%%inline\",tree_style=config[\"ete_tree_style\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering based on baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.162967Z",
     "start_time": "2019-08-08T12:37:24.347Z"
    }
   },
   "outputs": [],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Source prediction baseline\n",
    "print(\"Source word baseline tree\")\n",
    "strees = cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_source\", config=config, distance_col=2)\n",
    "for stree in strees:\n",
    "    print(stree)\n",
    "    display(stree.render(\"%%inline\",tree_style=config[\"ete_tree_style\"]))\n",
    "\n",
    "# PMI-based baseline\n",
    "print(\"PMI-based baseline tree\")\n",
    "ptrees = cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_pmi\", config=config, distance_col=3)\n",
    "for ptree in ptrees:\n",
    "    print(ptree)\n",
    "    display(ptree.render(\"%%inline\",tree_style=config[\"ete_tree_style\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Draw tree from existing newick string (no distance calculcation)\n",
    "This code can be used to draw trees as image, for which the newick string representation of the tree has already been generated by an earlier run of the clustering algorithm. A collection of previously generated tree strings can be found in the folder `data/trees`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree, TreeStyle, NodeStyle, TextFace\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# A number of sample newick strings, you can add your own here.\n",
    "slav_glottolog = \"((bel,rus,ukr),((hrv,slv),bul),((ces,slk),pol));\"\n",
    "slav_structperc_emb_nj = \"(((bel:0.07,ukr:0.11):0.02,rus:0.12):0.03,(((slv:0.11,hrv:0.1):0.02,bul:0.17):0.06,(pol:0.16,(ces:0.1,slk:0.06):0.06):0.04):0.03);\"\n",
    "slav_structperc_emb_upgma = \"((bul:0.15,(slv:0.1,hrv:0.1):0.05):0.04,((rus:0.11,(bel:0.09,ukr:0.09):0.02):0.07,(pol:0.15,(ces:0.08,slk:0.08):0.07):0.03):0.01);\"\n",
    "\n",
    "# If turned on, replace language code by language names from lang_dict\n",
    "USE_LANGUAGE_NAMES = True\n",
    "lang_dict = {\"rus\": \"Russian\", \"bel\":\"Belarusian\", \"ukr\":\"Ukrainian\", \"hrv\":\"Croatian\", \"slv\":\"Slovenian\", \"bul\":\"Bulgarian\", \"ces\":\"Czech\", \"slk\":\"Slovak\", \"pol\":\"Polish\"}\n",
    "\n",
    "\n",
    "for name,newick_string in [(\"slav_glottolog\", slav_glottolog),\n",
    "                           (\"slav_structperc_emb_nj\",slav_structperc_emb_nj),\n",
    "                           (\"slav_structperc_emb_upgma\",slav_structperc_emb_upgma)]:\n",
    "    if USE_LANGUAGE_NAMES:\n",
    "        for code in lang_dict:\n",
    "            newick_string = newick_string.replace(code, lang_dict[code])\n",
    "    if name==\"slav_glottolog\": # glottolog tree is without length\n",
    "        ts = TreeStyle()\n",
    "        ts.show_scale = False\n",
    "        ts.show_leaf_name = False\n",
    "        ts.force_topology = False\n",
    "        ts.show_border = False\n",
    "        ts.margin_top = ts.margin_bottom = ts.margin_right = ts.margin_left = 5\n",
    "        ts.scale = 50\n",
    "        ts.branch_vertical_margin= 10\n",
    "    else:\n",
    "        ts = TreeStyle()\n",
    "        ts.show_scale = False\n",
    "        ts.show_leaf_name = False\n",
    "        ts.force_topology = False\n",
    "        ts.show_border = False\n",
    "        ts.margin_top = ts.margin_bottom = ts.margin_right = ts.margin_left = 5\n",
    "        ts.scale = 500\n",
    "        ts.branch_vertical_margin= 10\n",
    "    # Load newick string into ete3 Tree object\n",
    "    tree = Tree(newick_string)\n",
    "    for node in tree.traverse():\n",
    "        node.set_style(config[\"ete_node_style\"])\n",
    "        if node.is_leaf():\n",
    "            # Add bit of extra space between leaf branch and leaf label\n",
    "            name_face = TextFace(f\" {node.name}\", fgcolor=\"black\", ftype=\"Charis SIL Compact\", fsize=11)\n",
    "            node.add_face(name_face, column=0, position='branch-right')\n",
    "    print(f\"output/tree_{name}.pdf\")\n",
    "    tree.render(f\"output/tree_{name}.pdf\", tree_style=ts)\n",
    "    display(tree.render(f\"%%inline\", tree_style=ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cognate detection\n",
    "To perform this step, the word prediction step has to be performed, with `cognate_detection=True` in the first initialization cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.163795Z",
     "start_time": "2019-08-08T12:37:26.572Z"
    }
   },
   "outputs": [],
   "source": [
    "from cognatedetection import cd\n",
    "\n",
    "print(\"Performing WP cognate detection using clustering...\")\n",
    "results_table = cd.cognate_detection_cluster(lang_pairs, config[\"results_dir\"], options, use_distance=\"prediction\")\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
