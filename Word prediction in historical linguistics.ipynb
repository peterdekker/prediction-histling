{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word prediction in historical linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T15:07:51.388051Z",
     "start_time": "2019-08-08T15:07:14.853998Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-08 17:07:16,111 [INFO] Successfully changed parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating all language pairs...\n",
      "Training corpus:\n",
      " - Loading dataset and performing necessary conversion/tokenization.\n",
      "Using existing wordlist file, nothing is generated.\n",
      " - Detect cognates in entire dataset using LexStat.\n",
      "Using existing cognates file output/northeuralex-asjp-cognates.tsv, nothing is generated.\n",
      "Validation/test corpus:\n",
      " - Loading dataset and performing necessary conversion/tokenization.\n",
      "Using existing wordlist file, nothing is generated.\n",
      " - Fetch list of concepts (only for valtest corpus)\n",
      " - Detect cognates in entire dataset using LexStat.\n",
      "Using existing cognates file output/northeuralex-asjp-cognates.tsv, nothing is generated.\n",
      "Create feature matrix for this specific language pair.\n",
      "Convert training corpus TSV file to data matrix\n",
      "Convert val/test corpus TSV file to data matrix\n",
      "USE TRAIN M/S\n",
      "Divide into training, validation and test set.\n",
      "Train/val/test sizes: 723|0|0\n",
      "Train/val/test sizes: 0|230|230\n",
      "Filter val/test sets on cognates.\n",
      "Val/test sizes after cognate filtering: 158|135\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/nld-deu.bat_siz10.bid_enc.cog_pri1.0.dro0.1.data/_phone_featu_new.t.fil_tra1.0.gru.gra_cli100.xavie_norma.chara.asjp.lea_rat0.01.lea_rat_dec1.0.n_epo15.n_hid400.n_ite_seq100.n_lay_dec1.n_lay_den1.n_lay_enc1.n_uni_phy400.adagr.chara.outpu.-data.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7c2733556595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                                                                \u001b[0mlanguages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                                                                                \u001b[0minput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"asjp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                                                                                options=options)\n\u001b[0m",
      "\u001b[0;32m~/git/prediction-histling/dataset/data.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(train_corpus, valtest_corpus, languages, input_type, options)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;31m# Pickle train/val/test/sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_pair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_pair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_pair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversion_key\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_pair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/nld-deu.bat_siz10.bid_enc.cog_pri1.0.dro0.1.data/_phone_featu_new.t.fil_tra1.0.gru.gra_cli100.xavie_norma.chara.asjp.lea_rat0.01.lea_rat_dec1.0.n_epo15.n_hid400.n_ite_seq100.n_lay_dec1.n_lay_den1.n_lay_enc1.n_uni_phy400.adagr.chara.outpu.-data.p'"
     ]
    }
   ],
   "source": [
    "from util import init\n",
    "from dataset import data\n",
    "from util.config import config\n",
    "\n",
    "# As user, you can either set separate languages or a language family\n",
    "languages = [\"nld\",\"deu\"]\n",
    "lang_family = None\n",
    "\n",
    "lang_family_dict = {\n",
    "\"slav\": [\"ces\", \"bul\", \"rus\", \"bel\", \"ukr\", \"pol\", \"slk\", \"slv\", \"hrv\"],\n",
    "\"ger\": [\"swe\", \"isl\", \"eng\", \"nld\", \"deu\", \"dan\", \"nor\"]\n",
    "}\n",
    "if lang_family:\n",
    "    languages = lang_family_dict[lang_family]\n",
    "\n",
    "options, distances_path, baselines_path = init.initialize_program()\n",
    "results_path, lang_pairs, train, val, test, max_len, conversion_key, voc_size = data.load_data(train_corpus=\"northeuralex\",\n",
    "                                                                                               valtest_corpus=\"northeuralex\",\n",
    "                                                                                               languages=languages,  \n",
    "                                                                                               input_type=\"asjp\", \n",
    "                                                                                               options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show number of cognates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.156995Z",
     "start_time": "2019-08-08T12:36:42.898Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Show number of cognates per language\")\n",
    "cog_per_lang, cliques = data.compute_n_cognates(lang_pairs, tsv_cognates_path_train, langs=config[\"languages\"], cognates_threshold=100)\n",
    "print(\"Cognates per language: \" + str(cog_per_lang))\n",
    "print(\"Number of cliques: \" + str(cliques))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform word prediction algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.157919Z",
     "start_time": "2019-08-08T12:37:07.979Z"
    }
   },
   "outputs": [],
   "source": [
    "for lang_pair in lang_pairs:\n",
    "    print(\"Performing RNN word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "    prediction.word_prediction(lang_a, lang_b, (max_len[lang_pair[0]], max_len[lang_pair[1]]), train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], voc_size, results_path[lang_pair], distances_path + \".txt\", context_vectors_path[lang_pair] + \".p\", config[\"output_encoding\"], config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word prediction using structured perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.158867Z",
     "start_time": "2019-08-08T12:37:12.451Z"
    }
   },
   "outputs": [],
   "source": [
    "for lang_pair in lang_pairs:\n",
    "    print(\"Performing structured perceptron word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "    prediction.word_prediction_seq(lang_a, lang_b, train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], results_path[lang_pair], distances_path + \".txt\", config)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.159715Z",
     "start_time": "2019-08-08T12:37:14.818Z"
    }
   },
   "outputs": [],
   "source": [
    "# Only ASJP\n",
    "for lang_pair in lang_pairs:\n",
    "    sounds = (list(features[0].index), list(features[1].index))\n",
    "    training_frame = train[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "    testing_frame = test[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "    baseline.compute_baseline(lang_a, lang_b, sounds, training_frame, testing_frame, baselines_path + \".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.160590Z",
     "start_time": "2019-08-08T12:37:17.651Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import data\n",
    "from visualize import visualize\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    # Create embedding for first languages\n",
    "    emb_matrix = data.create_embedding(lang_pair[0], [tsv_cognates_path_train + \".tsv\", tsv_cognates_path_valtest + \".tsv\"])\n",
    "    visualize.visualize_encoding(emb_matrix, feature_matrix_phon, lang_pair, config[\"results_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify sound correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.161412Z",
     "start_time": "2019-08-08T12:37:20.235Z"
    }
   },
   "outputs": [],
   "source": [
    "from visualize import visualize\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    visualize.show_output_substitutions(results_path[lang_pair], subs_st_path[lang_pair], subs_sp_path[lang_pair])\n",
    "    visualize.visualize_weights(context_vectors_path[lang_pair], lang_pair, config[\"input_encoding\"], config[\"output_encoding\"], config[\"results_dir\"], sample=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phylogenetic tree reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering based on word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.162205Z",
     "start_time": "2019-08-08T12:37:22.524Z"
    }
   },
   "outputs": [],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Cluster based on word prediction distances\n",
    "print(\"WP TREE:\\n\")\n",
    "cluster.cluster_languages(lang_pairs, distances_path, output_path=distances_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.162967Z",
     "start_time": "2019-08-08T12:37:24.347Z"
    }
   },
   "outputs": [],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Source prediction baseline\n",
    "print(\"\\nSOURCE BASELINE TREE\")\n",
    "cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_source\", distance_col=2)\n",
    "# PMI-based baseline\n",
    "print(\"\\nPMI BASELINE TREE\")\n",
    "cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_pmi\", distance_col=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cognate detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.163795Z",
     "start_time": "2019-08-08T12:37:26.572Z"
    }
   },
   "outputs": [],
   "source": [
    "from cognatedetection import cd\n",
    "\n",
    "### TODO: this part from previous code should not be executed:\n",
    "# print(\"Filter val/test sets on cognates.\")\n",
    "# Use only cognate pairs for validation and test\n",
    "# val[lang_pair] = val[lang_pair].filter_cognates()\n",
    "# test[lang_pair] = test[lang_pair].filter_cognates()\n",
    "# print(\"Val/test sizes after cognate filtering: \" + str(val[lang_pair].get_size()) + \"|\" + str(test[lang_pair].get_size()))\n",
    "\n",
    "\n",
    "print(\"Performing WP cognate detection using clustering...\")\n",
    "results_table = cd.cognate_detection_cluster(lang_pairs, config[\"results_dir\"], options, use_distance=\"prediction\")\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phylogenetic word prediction (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.164558Z",
     "start_time": "2019-08-08T12:37:28.854Z"
    }
   },
   "outputs": [],
   "source": [
    "from util import utility\n",
    "from prediction import prediction\n",
    "\n",
    "\n",
    "# In phylogenetic mode, we created one feature matrix for all languages\n",
    "for lang_pair in lang_pairs:\n",
    "    conversion_key[lang_pair] = conversion_key_general\n",
    "\n",
    "voc_size = voc_size_general\n",
    "# For phylogenetic word prediction, create one feature matrix for all languages\n",
    "print(\"Create feature matrix for all language pairs.\")\n",
    "used_tokens = [[], []]\n",
    "tokens_set = [[], []]\n",
    "for lang_pair in lang_pairs:\n",
    "    # For phylogenetic word prediction, create one feature matrix for all languages\n",
    "    features_lp[lang_pair], max_len[lang_pair[0]], max_len[lang_pair[1]], _, _ = data.get_corpus_info([tsv_cognates_path_train + \".tsv\", tsv_cognates_path_valtest + \".tsv\"], lang_pair=lang_pair, input_encoding=config[\"input_encoding\"], output_encoding=config[\"output_encoding\"], feature_matrix_phon=feature_matrix_phon)\n",
    "    used_tokens[0] += list(features_lp[lang_pair][0].index)\n",
    "    used_tokens[1] += list(features_lp[lang_pair][1].index)\n",
    "\n",
    "tokens_set[0] = list(set(used_tokens[0]))\n",
    "tokens_set[1] = list(set(used_tokens[1]))\n",
    "if config[\"input_encoding\"] == \"character\":\n",
    "    features[0] = data.create_one_hot_matrix(tokens_set[0])\n",
    "elif config[\"input_encoding\"] == \"phonetic\":\n",
    "    features[0] = feature_matrix_phon.loc[tokens_set[0]]\n",
    "else:\n",
    "    print(\"Embedding encoding not possible in phylogenetic tree prediction.\")\n",
    "    return\n",
    "# Output encoding is always character\n",
    "features[1] = data.create_one_hot_matrix(tokens_set[1])\n",
    "voc_size_general[0] = features[0].shape[1]\n",
    "voc_size_general[1] = features[1].shape[1]\n",
    "conversion_key_general = data.create_conversion_key(features)\n",
    "plot_path_phyl = utility.create_path(config[\"results_dir\"], options, prefix=\"plot_\")\n",
    "\n",
    "config[\"export_weights\"] = False  # Turn off export of weights\n",
    "tree_string = \"((nld,deu),eng)\"  # unused at the moment\n",
    "if len(config[\"languages\"]) >= 3:\n",
    "    results_path_proto = utility.create_path(config[\"results_dir\"], options, prefix=\"proto_\")  # lang-pair independent path\n",
    "    prediction.word_prediction_phyl(config[\"languages\"], lang_pairs, tree_string, max_len, train, val, test, conversion_key_general, voc_size, results_path, results_path_proto, distances_path + \".txt\", context_vectors_path, plot_path_phyl, config[\"output_encoding\"], config)\n",
    "else:\n",
    "    print(\"Please supply 3 languages, the first 2 being more closely related than the last.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
