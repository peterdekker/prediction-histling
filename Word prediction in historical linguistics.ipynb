{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word prediction in historical linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application and data loading\n",
    "Give a list of languages (from [NorthEuraLex](http://northeuralex.org/languages)) to take under consideration in the `languages` variable. Or set `lang_family` to a language family: `slav` or `ger`. If `lang_family` has been set (not `None`), `languages` will be disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T15:07:51.388051Z",
     "start_time": "2019-08-08T15:07:14.853998Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 14:26:49,486 [INFO] Successfully changed parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing program...\n",
      "Loading phonetic feature matrix...\n",
      "Generating all language pairs...\n",
      "Training corpus:\n",
      " - Loading dataset and performing necessary conversion/tokenization.\n",
      "Using existing wordlist file, nothing is generated.\n",
      " - Detect cognates in entire dataset using LexStat.\n",
      "Using existing cognates file output/northeuralex-asjp-cognates.tsv, nothing is generated.\n",
      "Train corpus is valtest corpus.\n",
      "Creating feature matrix for this specific language pair...\n",
      "Converting training corpus TSV file to data matrix...\n",
      "Converting val/test corpus TSV file to data matrix...\n",
      "USE TRAIN M/S\n",
      "Dividing into training, validation and test set...\n",
      "Train/val/test sizes: 711|0|0\n",
      "Train/val/test sizes: 0|236|236\n",
      "Filtering val/test sets on cognates...\n",
      "Val/test sizes after cognate filtering: 159|139\n",
      "Creating feature matrix for this specific language pair...\n",
      "Converting training corpus TSV file to data matrix...\n",
      "Converting val/test corpus TSV file to data matrix...\n",
      "USE TRAIN M/S\n",
      "Dividing into training, validation and test set...\n",
      "Train/val/test sizes: 830|0|0\n",
      "Train/val/test sizes: 0|276|276\n",
      "Filtering val/test sets on cognates...\n",
      "Val/test sizes after cognate filtering: 118|135\n",
      "Creating feature matrix for this specific language pair...\n",
      "Converting training corpus TSV file to data matrix...\n",
      "Converting val/test corpus TSV file to data matrix...\n",
      "USE TRAIN M/S\n",
      "Dividing into training, validation and test set...\n",
      "Train/val/test sizes: 828|0|0\n",
      "Train/val/test sizes: 0|275|275\n",
      "Filtering val/test sets on cognates...\n",
      "Val/test sizes after cognate filtering: 66|86\n",
      "Creating feature matrix for this specific language pair...\n",
      "Converting training corpus TSV file to data matrix...\n",
      "Converting val/test corpus TSV file to data matrix...\n",
      "USE TRAIN M/S\n",
      "Dividing into training, validation and test set...\n",
      "Train/val/test sizes: 711|0|0\n",
      "Train/val/test sizes: 0|236|236\n",
      "Filtering val/test sets on cognates...\n",
      "Val/test sizes after cognate filtering: 159|139\n",
      "Creating feature matrix for this specific language pair...\n",
      "Converting training corpus TSV file to data matrix...\n",
      "Converting val/test corpus TSV file to data matrix...\n",
      "USE TRAIN M/S\n",
      "Dividing into training, validation and test set...\n",
      "Train/val/test sizes: 702|0|0\n",
      "Train/val/test sizes: 0|234|234\n",
      "Filtering val/test sets on cognates...\n",
      "Val/test sizes after cognate filtering: 126|108\n",
      "Creating feature matrix for this specific language pair...\n",
      "Converting training corpus TSV file to data matrix...\n",
      "Converting val/test corpus TSV file to data matrix...\n",
      "USE TRAIN M/S\n",
      "Dividing into training, validation and test set...\n",
      "Train/val/test sizes: 704|0|0\n",
      "Train/val/test sizes: 0|234|234\n",
      "Filtering val/test sets on cognates...\n",
      "Val/test sizes after cognate filtering: 64|76\n",
      "Creating feature matrix for this specific language pair...\n",
      "Converting training corpus TSV file to data matrix...\n",
      "Converting val/test corpus TSV file to data matrix...\n",
      "USE TRAIN M/S\n",
      "Dividing into training, validation and test set...\n",
      "Train/val/test sizes: 830|0|0\n",
      "Train/val/test sizes: 0|276|276\n",
      "Filtering val/test sets on cognates...\n",
      "Val/test sizes after cognate filtering: 117|137\n",
      "Creating feature matrix for this specific language pair...\n",
      "Converting training corpus TSV file to data matrix...\n",
      "Converting val/test corpus TSV file to data matrix...\n",
      "USE TRAIN M/S\n",
      "Dividing into training, validation and test set...\n",
      "Train/val/test sizes: 702|0|0\n",
      "Train/val/test sizes: 0|234|234\n",
      "Filtering val/test sets on cognates...\n",
      "Val/test sizes after cognate filtering: 126|108\n",
      "Creating feature matrix for this specific language pair...\n",
      "Converting training corpus TSV file to data matrix...\n",
      "Converting val/test corpus TSV file to data matrix...\n",
      "USE TRAIN M/S\n",
      "Dividing into training, validation and test set...\n",
      "Train/val/test sizes: 824|0|0\n",
      "Train/val/test sizes: 0|274|274\n",
      "Filtering val/test sets on cognates...\n",
      "Val/test sizes after cognate filtering: 97|107\n",
      "Creating feature matrix for this specific language pair...\n",
      "Converting training corpus TSV file to data matrix...\n",
      "Converting val/test corpus TSV file to data matrix...\n",
      "USE TRAIN M/S\n",
      "Dividing into training, validation and test set...\n",
      "Train/val/test sizes: 828|0|0\n",
      "Train/val/test sizes: 0|275|275\n",
      "Filtering val/test sets on cognates...\n",
      "Val/test sizes after cognate filtering: 68|83\n",
      "Creating feature matrix for this specific language pair...\n",
      "Converting training corpus TSV file to data matrix...\n",
      "Converting val/test corpus TSV file to data matrix...\n",
      "USE TRAIN M/S\n",
      "Dividing into training, validation and test set...\n",
      "Train/val/test sizes: 704|0|0\n",
      "Train/val/test sizes: 0|234|234\n",
      "Filtering val/test sets on cognates...\n",
      "Val/test sizes after cognate filtering: 64|76\n",
      "Creating feature matrix for this specific language pair...\n",
      "Converting training corpus TSV file to data matrix...\n",
      "Converting val/test corpus TSV file to data matrix...\n",
      "USE TRAIN M/S\n",
      "Dividing into training, validation and test set...\n",
      "Train/val/test sizes: 824|0|0\n",
      "Train/val/test sizes: 0|274|274\n",
      "Filtering val/test sets on cognates...\n",
      "Val/test sizes after cognate filtering: 99|107\n",
      "Done loading data.\n"
     ]
    }
   ],
   "source": [
    "from util import init\n",
    "from dataset import data\n",
    "from util.config import config\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "lang_family_dict = {\n",
    "\"slav\": [\"ces\", \"bul\", \"rus\", \"bel\", \"ukr\", \"pol\", \"slk\", \"slv\", \"hrv\"],\n",
    "\"ger\": [\"swe\", \"isl\", \"eng\", \"nld\", \"deu\", \"dan\", \"nor\"]\n",
    "}\n",
    "\n",
    "# As user, you can either set separate languages or a language family\n",
    "languages = [\"nld\",\"deu\", \"swe\", \"isl\"]\n",
    "lang_family = None\n",
    "\n",
    "\n",
    "if lang_family:\n",
    "    languages = lang_family_dict[lang_family]\n",
    "\n",
    "options, distances_path, baselines_path = init.initialize_program()\n",
    "(results_path, output_path_cognates_train, output_path_cognates_valtest,\n",
    "context_vectors_path, subs_sp_path, subs_st_path, lang_pairs, train, val, test, max_len, \n",
    "conversion_key, voc_size, feature_matrix_phon) = data.load_data(train_corpus=\"northeuralex\",\n",
    "                                                               valtest_corpus=\"northeuralex\",\n",
    "                                                               languages=languages,  \n",
    "                                                               input_type=\"asjp\", \n",
    "                                                               options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Enable weight visualization\n",
    "Run the following cell, to be able to export and visualize the weights of the network. Do this only for diagnostic/visualization purposes, because the batch size will be set to 1, which is not optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"export_weights\"]=True\n",
    "config[\"batch_size\"]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize encoding\n",
    "Visualize the representation of phonemes in the embedding encoding, as PCA and as hierarchically clustered tree. Compare them to the phonetic feature matrix from Brown (2008)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.160590Z",
     "start_time": "2019-08-08T12:37:17.651Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import data\n",
    "from visualize import visualize\n",
    "from tree import cluster\n",
    "\n",
    "tree_style = config[\"ete_tree_style\"]\n",
    "\n",
    "print(\"Phonetic matrix from Brown (2008):\")\n",
    "# Perform PCA on phonetic feature matrix from Brown (2008)\n",
    "phon_matrix_red, phon_phonemes = visualize.dim_reduction(feature_matrix_phon)\n",
    "# Visualize phonetic feature PCA using plot\n",
    "visualize.visualize_encoding(phon_matrix_red, phon_phonemes, \"phonetic-pca\")\n",
    "# Hierarchically cluster distances between phonemes in phonetic feature matrix\n",
    "tree = cluster.cluster_phonemes_encoding(feature_matrix_phon, phon_phonemes, \"phonetic\")\n",
    "\n",
    "display(tree.render(\"%%inline\", tree_style=tree_style))\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    lang_a = lang_pair[0]\n",
    "    print(f\"Embedding for {lang_a}:\")\n",
    "    # Create embedding for every first language in language pair\n",
    "    emb_matrix = data.create_embedding(lang_a, [output_path_cognates_train, output_path_cognates_valtest])\n",
    "    # Perform PCA on embedding matrix\n",
    "    emb_matrix_red, emb_phonemes = visualize.dim_reduction(emb_matrix)\n",
    "    # Visualize embedding PCA using plot\n",
    "    visualize.visualize_encoding(emb_matrix_red, emb_phonemes, f\"embedding-{lang_pair[0]}-pca\")\n",
    "    \n",
    "    # Hierarchically cluster distances between phonemes in embedding matrix\n",
    "    tree = cluster.cluster_phonemes_encoding(emb_matrix, emb_phonemes, f\"embedding-{lang_pair[0]}\")\n",
    "    display(tree.render(\"%%inline\", tree_style=tree_style))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close-up: constrast occurrences of phonemes in data\n",
    "Interesting patterns in the phoneme encoding visualizations, can be looked up in the data. In the Dutch embedding encoding, we saw that *t* and *d*, closely related phonemes, are quite remote in the embedding space. How do the words with *t* and *d*, on which the embedding encoding is based, look in Dutch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lang = \"nld\"\n",
    "# Read in TSV file with data\n",
    "df = pd.read_csv(output_path_cognates_train, sep=\"\\t\", engine=\"python\", skipfooter=3, index_col=False)\n",
    "df_lang = df[df[\"DOCULECT\"] == lang]\n",
    "\n",
    "for phoneme in [\"d\", \"t\"]:\n",
    "    print(phoneme)\n",
    "    words_with_phoneme = df_lang[df_lang[\"ASJP\"].str.contains(phoneme)]\n",
    "    total = len(words_with_phoneme)\n",
    "    print(f\"Total number of occurrences: {total}\")\n",
    "    \n",
    "    # Compute locations of phonemes in word\n",
    "    locations = df_lang[\"ASJP\"].str.find(phoneme)\n",
    "    locations = locations[locations != -1]\n",
    "    # Compute relative frequencies\n",
    "    locations_relfreq = locations.value_counts(normalize=True)\n",
    "    print(\"Relative frequencies of locations:\")\n",
    "    print(locations_relfreq)\n",
    "    # Look up words with most frequent location\n",
    "    most_freq_location = int(locations.mode())\n",
    "    words_in_most_freq_loc = words_with_phoneme[words_with_phoneme[\"ASJP\"].str.find(phoneme) == most_freq_location]\n",
    "    print(f\"Words with {phoneme} where {phoneme} has most frequent location in word ({most_freq_location}):\")\n",
    "    print(words_in_most_freq_loc)\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "#words_list = list(df_lang[\"TOKENS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Show number of cognates in training data\n",
    "Show the number of cognate word pairs per language pair in the training data, and calculate cliques of languages with a minimum of 100 shared cognates. These cliques can later be used, to have a group of languages with a large shared number of cognates, to perform prediction on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.156995Z",
     "start_time": "2019-08-08T12:36:42.898Z"
    }
   },
   "outputs": [],
   "source": [
    "cog_per_lang, cliques = data.compute_n_cognates(lang_pairs, output_path_cognates_train, langs=languages, cognates_threshold=100)\n",
    "print(\"Cognates per language: \")\n",
    "print((cog_per_lang))\n",
    "print(\"Cliques: \")\n",
    "for c in cliques:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word prediction using structured perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.158867Z",
     "start_time": "2019-08-08T12:37:12.451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing structured perceptron word prediction for pair (nld, deu)\n",
      "Create SeqModel instance.\n",
      "Train sequential model.\n",
      "Training ...\n",
      "Duration = 42.15\n",
      "Predict using sequential model and show results.\n",
      "INPUT                TARGET               PREDICTION           DISTANCE\n",
      "tant3                tant3                canc3                0.40\n",
      "til3                 hoGheb3n             cil3                 0.88\n",
      "vErkiz3              vEl3n                fErkiz3              0.86\n",
      "kid3                 hErd3                kG3n                 1.00\n",
      "stErkt3              StErk3               StErk3n              0.14\n",
      "krEis                kGoic                kGaiS                0.40\n",
      "brur                 bGuda                bGuG                 0.40\n",
      "morx3                morg3n               morg3                0.17\n",
      "sam3                 cuzam3n              Sam3                 0.57\n",
      "vErxan               fErge3n              fErg3n               0.14\n",
      "klim3                klEtan               klim3                0.67\n",
      "bind3l               bind3l               b3nd3l               0.17\n",
      "link3r               liNka                liNk3n               0.33\n",
      "won3                 von3n                von                  0.40\n",
      "sx3lt                Sult                 Sp3lt                0.40\n",
      "brand3               bGEn3n               bGand3               0.50\n",
      "ski                  ski                  S3n                  1.00\n",
      "zikzEin              kGaNkzain            ziSzain              0.56\n",
      "drain                dGe3n                dGain                0.40\n",
      "rex3                 Geg3n                Geg3                 0.20\n",
      "sid3r3               citan                Sid3G3               0.83\n",
      "hElft                hElft3               hElft                0.17\n",
      "ovErwin3             zig3n                ufErvin              0.86\n",
      "fert3x               firciS               fErc3n               0.50\n",
      "tek3                 caiS3n               ce3n                 0.50\n",
      "nakt                 nakt                 nait                 0.25\n",
      "hoNerix              huNGiS               hoNErha              0.71\n",
      "wrEiv3               Gaib3n               vErg3n               0.67\n",
      "zond3                zind3                zond3                0.20\n",
      "zixvErzam3l3         ziSfErzam3ln         ziSfErzam3l3         0.08\n",
      "hoN3r                huNa                 hoN3n                0.60\n",
      "zom3r                zoma                 zom3n                0.40\n",
      "hart                 hErc                 hart                 0.50\n",
      "b3drix3              b3tGig3n             b3dGig3              0.25\n",
      "drEk                 dGEk                 dGEk                 0.00\n",
      "hals                 hals                 halS                 0.25\n",
      "pat                  pat                  fat                  0.33\n",
      "spul3                Spil3n               Spul3                0.33\n",
      "hEmt                 hEmt                 hemt                 0.25\n",
      "krEip3               kGiS3n               kGaip3               0.50\n",
      "b3tal3               b3cal3n              b3nal3               0.29\n",
      "t3r3x                vida                 c3GiS                1.00\n",
      "vex3                 feg3n                feg3                 0.20\n",
      "x3wixt               g3viSt               g3viSt               0.00\n",
      "lEid3                loit3n               lEc3n                0.50\n",
      "spits                Spic                 Spist                0.40\n",
      "hev3l                hig3l                heb3l                0.40\n",
      "xord3l               girt3l               gord3l               0.33\n",
      "b3hers3              b3hErS3n             b3neG3n              0.50\n",
      "rEn3                 GEn3n                G3n                  0.40\n",
      "bEix3                boig3n               baig3                0.33\n",
      "kis3                 kis3n                kiS3                 0.40\n",
      "buzEm                buz3n                buzim                0.40\n",
      "inslap3              ainSlaf3n            ziSlaun              0.44\n",
      "ton                  ton                  cun                  0.67\n",
      "mid3                 mit3                 mid3                 0.25\n",
      "wort3l               vurc3l               vorc3l               0.17\n",
      "vremt                fGEmt                fGemt                0.20\n",
      "strom3               StGem3n              StGum3               0.29\n",
      "zid3                 zid3n                zid3                 0.20\n",
      "slEip3               Slaif3n              Slaip3               0.29\n",
      "yini                 yuni                 yun                  0.25\n",
      "handuk               hantuG               handuk               0.33\n",
      "x3borEnword3         g3boGenvErd3n        geborb3vuG3n         0.54\n",
      "x3zin                famili               g3zin                0.83\n",
      "rot                  Got                  Got                  0.00\n",
      "xEf                  SEf                  g3n                  1.00\n",
      "nat                  nas                  nat                  0.33\n",
      "hem3l                him3l                hem3l                0.20\n",
      "xev3                 geb3n                geb3                 0.20\n",
      "x3lEit               laut                 g3lait               0.50\n",
      "lex                  ler                  liS                  0.67\n",
      "wExan                vEge3n               vig3n                0.33\n",
      "vrau                 fGau                 fGal                 0.25\n",
      "stot3                Stos3n               Stos3                0.17\n",
      "vErbet3r3            fErbEsan             fErbec3G3            0.56\n",
      "b3xin3               anfaN3n              b3g3n                0.71\n",
      "vErxet3              fErgEs3n             fErge3n              0.25\n",
      "der                  tir                  dor                  0.67\n",
      "stan                 Ste3n                Stan                 0.40\n",
      "kok3                 koG3n                koin                 0.40\n",
      "sxEin3               Sain3n               SErg3n               0.50\n",
      "uv3r                 ufa                  uf3n                 0.50\n",
      "Eil3                 ail3n                ail3                 0.20\n",
      "Eiz3r                aiz3n                aiz3n                0.00\n",
      "tal                  ancal                cal                  0.40\n",
      "sxer3                Gais3n               SleG3                0.83\n",
      "part                 pErt                 fErt                 0.25\n",
      "vuts3l               naGuN                fust3l               1.00\n",
      "blint                blint                blint                0.00\n",
      "afsnEid3             abSnaid3n            ab3naid3             0.22\n",
      "vErbrand3            fErbGEn3n            fErbGand3            0.33\n",
      "anzin                anze3n               anzin                0.33\n",
      "warhEit              varhait              varhait              0.00\n",
      "rEip                 Gaif                 Gaip                 0.25\n",
      "vur3                 fitan                fuG3                 0.80\n",
      "rur3                 GiG3n                GuG3                 0.40\n",
      "hErfst               hErbst               hErbst               0.00\n",
      "wEnd3                vEnd3n               vEnd3                0.17\n",
      "tuvux3               hincufig3n           cufug3               0.50\n",
      "vex3                 keG3n                feg3                 0.60\n",
      "mEis                 maus                 maiS                 0.50\n",
      "x3wer                geveG                g3vEr                0.60\n",
      "vol                  fol                  fol                  0.00\n",
      "blaz3                blaz3n               blez3                0.33\n",
      "stEix3               Staig3n              Staig3               0.14\n",
      "stok                 Stok                 Stok                 0.00\n",
      "morx3                morg3n               morg3                0.17\n",
      "stomp                Stump                Stomp                0.20\n",
      "inwik3l3             ainvik3ln            g3vik3l3             0.44\n",
      "reparer3             GepaGiG3n            GapaGeG3             0.33\n",
      "dri                  dGai                 dGi                  0.25\n",
      "dr3p3l               tGop3n               dGip3l               0.50\n",
      "hEld3r               hEl                  hElt3n               0.50\n",
      "sx3im                Saum                 S3nim                0.60\n",
      "last                 last                 last                 0.00\n",
      "dir                  tir                  dor                  0.67\n",
      "aNelExEnhEit         aNelegEnhait         aNElig3nhait         0.25\n",
      "hondErt              hundErt              hondErt              0.14\n",
      "rup3                 Guf3n                Guf3                 0.20\n",
      "drom                 tGaum                dGum                 0.40\n",
      "tudEk3               cudEk3n              cviNk3               0.57\n",
      "brad3                bGat3n               bGain                0.33\n",
      "ost3                 ost3n                ost3                 0.20\n",
      "slik3                Sluk3n               Slik3                0.33\n",
      "lerar                leGa                 leGar                0.20\n",
      "mar                  nur                  mar                  0.67\n",
      "xrun                 gGin                 gGun                 0.25\n",
      "hErkEn3              ErkEn3n              hErk3n               0.43\n",
      "spel3                Spil3n               Spal3                0.33\n",
      "vErbind3             fErbind3n            fErb3nd3             0.22\n",
      "mart                 mErc                 mart                 0.50\n",
      "arm                  arm                  arm                  0.00\n",
      "sprek3               SpGES3n              SpGe3n               0.29\n",
      "stap3                SGait3n              Staun                0.57\n",
      "zwErm                Svarm                SvErm                0.20\n",
      "lint                 bant                 lint                 0.50\n",
      "z3                   zi                   z3                   0.50\n",
      "r3is3                GauS3n               GaiS3                0.33\n",
      "Average distance: 0.3881863739777409\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing structured perceptron word prediction for pair (nld, swe)\n",
      "Create SeqModel instance.\n",
      "Train sequential model.\n",
      "Training ...\n"
     ]
    }
   ],
   "source": [
    "from prediction import prediction\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    lang_a, lang_b = lang_pair\n",
    "    print(\"Performing structured perceptron word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "    prediction.word_prediction_seq(lang_a, lang_b, train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], results_path[lang_pair], distances_path + \".txt\", config)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word prediction using encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.157919Z",
     "start_time": "2019-08-08T12:37:07.979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 26]\n",
      "Performing RNN word prediction for pair (nld, deu)\n",
      "Create RNN instance.\n",
      "Building network ...\n",
      "Creating loss function...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5b2de6283240>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlang_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlang_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang_pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Performing RNN word prediction for pair (\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlang_a\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlang_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\")\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_pair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_pair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_pair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversion_key\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_pair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_pair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_vectors_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_pair\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_encoding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/prediction-histling/prediction/prediction.py\u001b[0m in \u001b[0;36mword_prediction\u001b[0;34m(lang_a, lang_b, max_len, train, val, test, conversion_key, voc_size, results_path, distances_path, context_vectors_path, output_encoding, config)\u001b[0m\n\u001b[1;32m     35\u001b[0m                          \u001b[0mconversion_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconversion_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                          \u001b[0mexport_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"export_weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                          optimizer=config[\"optimizer\"])\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Both prediction and validation rounds during training are performed on 'testset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/prediction-histling/models/EncoderDecoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, max_len, voc_size, n_hidden, n_layers_encoder, n_layers_decoder, n_layers_dense, bidirectional_encoder, bidirectional_decoder, encoder_only_final, dropout, learning_rate, learning_rate_decay, adaptive_learning_rate, reg_weight, grad_clip, initialization, gated_layer_type, cognacy_prior, input_encoding, output_encoding, conversion_key, export_weights, optimizer)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Theano functions for training and computing loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Compiling functions ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_in_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_target_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_mask_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_values_determ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicted_values_deterministic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresetgate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdategate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddengate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresetgate_fw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdategate_fw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddengate_fw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_comb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/prediction-histling/models/EncoderDecoder.py\u001b[0m in \u001b[0;36m_compile_functions\u001b[0;34m(self, X_input, Y_input, mask, error_threshold, loss, updates, pred_values_determ, context_vector)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             train_func = theano.function([X_input, Y_input, mask, error_threshold, self.lr_var],\n\u001b[0;32m--> 266\u001b[0;31m                                     [loss[0], loss[1], loss[2]], updates=updates, on_unused_input=\"warn\")\n\u001b[0m\u001b[1;32m    267\u001b[0m             loss_func = theano.function(\n\u001b[1;32m    268\u001b[0m                                     \u001b[0;34m[\u001b[0m\u001b[0mX_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_threshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    315\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1837\u001b[0m                   \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m                   \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m                   name=name)\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_test_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys, name)\u001b[0m\n\u001b[1;32m   1517\u001b[0m                         optimizer, inputs, outputs)\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m                     \u001b[0moptimizer_profile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0mend_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fgraph)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fgraph)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0mnb_nodes_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                     \u001b[0msub_prof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                     \u001b[0msub_profs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_prof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fgraph)\u001b[0m\n\u001b[1;32m    899\u001b[0m                     \u001b[0;31m# If it is a Constant, the other must also be a Constant as we merge them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConstant\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                         \u001b[0mfgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MergeOptimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                         \u001b[0mfgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_all_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MergeOptimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/gof/fg.py\u001b[0m in \u001b[0;36mreplace_all\u001b[0;34m(self, pairs, reason)\u001b[0m\n\u001b[1;32m    525\u001b[0m         \"\"\"\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_r\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mattach_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/gof/fg.py\u001b[0m in \u001b[0;36mreplace\u001b[0;34m(self, r, new_r, reason, verbose)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# copy the client list for iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'output'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;31m# sometimes the following is triggered.  If you understand why, please explain to James.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/gof/fg.py\u001b[0m in \u001b[0;36mchange_input\u001b[0;34m(self, node, i, new_r, reason)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;31m# transaction will be reverted later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         self.execute_callbacks('on_change_input', node, i,\n\u001b[0;32m--> 452\u001b[0;31m                                r, new_r, reason=reason)\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;31m# replace #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/gof/fg.py\u001b[0m in \u001b[0;36mexecute_callbacks\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0mtf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_callbacks_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_callbacks_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mon_change_input\u001b[0;34m(self, fgraph, node, i, r, new_r, reason)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes_seen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes_seen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;31m# Since we are in on_change_input, node should have inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mprocess_node\u001b[0;34m(self, fgraph, node)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;31m# and -1 speed up optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m                 \u001b[0mclients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from prediction import prediction\n",
    "print(voc_size)\n",
    "for lang_pair in lang_pairs:\n",
    "    lang_a,lang_b = lang_pair\n",
    "    print(\"Performing RNN word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "    prediction.word_prediction_rnn(lang_a, lang_b, (max_len[lang_a], max_len[lang_b]), train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], voc_size, results_path[lang_pair], distances_path + \".txt\", context_vectors_path[lang_pair] + \".p\", config[\"output_encoding\"], config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.159715Z",
     "start_time": "2019-08-08T12:37:14.818Z"
    }
   },
   "outputs": [],
   "source": [
    "from models import baseline\n",
    "\n",
    "# Only ASJP\n",
    "for lang_pair in lang_pairs:\n",
    "    lang_a,lang_b = lang_pair\n",
    "    conv = conversion_key[lang_pair]\n",
    "    sounds = (list(conv[0].values()), list(conv[1].values()))\n",
    "    training_frame = train[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "    testing_frame = test[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "    baseline.compute_baseline(lang_a, lang_b, sounds, training_frame, testing_frame, baselines_path + \".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify sound correspondences\n",
    "##### Based on output substitutions\n",
    "This shows the substitutions table in the notebook, and outputs it as LaTeX table to `RESULTS_DIR/subs.tex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.161412Z",
     "start_time": "2019-08-08T12:37:20.235Z"
    }
   },
   "outputs": [],
   "source": [
    "from visualize import visualize\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    visualize.show_output_substitutions(results_path[lang_pair], subs_st_path[lang_pair], subs_sp_path[lang_pair])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on context vector weights (only when weight exported activated)\n",
    "This works when word prediction with encoder-decoder has been run, and weight export has been activated at the top of this notebook. Executing this command shows the distances between input, output and context vectors. Furthermore, it generates PCA plots of the input, output and context vectors and generates LaTeX files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import visualize\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    visualize.visualize_weights(context_vectors_path[lang_pair], lang_pair, config[\"input_encoding\"], config[\"output_encoding\"], config[\"results_dir\"], sample=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phylogenetic tree reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering based on word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.162205Z",
     "start_time": "2019-08-08T12:37:22.524Z"
    }
   },
   "outputs": [],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Cluster based on word prediction distances\n",
    "print(\"WP TREE:\\n\")\n",
    "cluster.cluster_languages(lang_pairs, distances_path, output_path=distances_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering based on baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.162967Z",
     "start_time": "2019-08-08T12:37:24.347Z"
    }
   },
   "outputs": [],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Source prediction baseline\n",
    "print(\"\\nSOURCE BASELINE TREE\")\n",
    "cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_source\", distance_col=2)\n",
    "# PMI-based baseline\n",
    "print(\"\\nPMI BASELINE TREE\")\n",
    "cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_pmi\", distance_col=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Draw tree from existing newick string (no distance calculcation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Improvised code to re-generate trees\n",
    "from ete3 import Tree, TreeStyle, NodeStyle, TextFace\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "newick_string1 = \"((bul:0.15,(slv:0.11,hrv:0.11):0.04):0.04,((rus:0.11,(bel:0.1,ukr:0.1):0.01):0.07,(pol:0.15,(ces:0.08,slk:0.08):0.07):0.03):0.01);\"\n",
    "newick_string2 = \"(((bel:0.08,ukr:0.12):0.01,rus:0.12):0.03,(((slv:0.1,hrv:0.11):0.01,bul:0.18):0.07,(pol:0.17,(ces:0.1,slk:0.07):0.05):0.04):0.03);\"\n",
    "newick_string3 = \"((bul:0.29,((ces:0.24,slk:0.24):0.04,(slv:0.25,hrv:0.25):0.03):0.01):0.01,(pol:0.28,(rus:0.24,(bel:0.22,ukr:0.22):0.02):0.04):0.01);\"\n",
    "newick_string4 = \"((bel,rus,ukr),((hrv,slv),bul),((ces,slk),pol));\"\n",
    "\n",
    "ts = TreeStyle()\n",
    "ts.show_scale = False\n",
    "ts.show_leaf_name = False\n",
    "ts.force_topology = False\n",
    "ts.show_border = False\n",
    "ts.margin_top = ts.margin_bottom = ts.margin_right = ts.margin_left = 5\n",
    "ts.scale = 500\n",
    "ts.branch_vertical_margin= 10\n",
    "\n",
    "\n",
    "for i,newick_string in enumerate([newick_string1, newick_string2, newick_string3, newick_string4]):\n",
    "    if i==3: # last newick string without lengths, should be corrected\n",
    "        ts = TreeStyle()\n",
    "        ts.show_scale = False\n",
    "        ts.show_leaf_name = False\n",
    "        ts.force_topology = False\n",
    "        ts.show_border = False\n",
    "        ts.margin_top = ts.margin_bottom = ts.margin_right = ts.margin_left = 5\n",
    "        ts.scale = 50\n",
    "        ts.branch_vertical_margin= 10\n",
    "    # Load newick string into ete3 Tree object\n",
    "    tree = Tree(newick_string)\n",
    "    for node in tree.traverse():\n",
    "        node.set_style(config[\"ete_node_style\"])\n",
    "        if node.is_leaf():\n",
    "            # Add bit of extra space between leaf branch and leaf label\n",
    "            name_face = TextFace(f\" {node.name}\", fgcolor=\"black\", fsize=10)\n",
    "            node.add_face(name_face, column=0, position='branch-right')\n",
    "    print(f\"output/tree{i+1}.pdf\")\n",
    "    tree.render(f\"output/tree{i+1}.pdf\", tree_style=ts)\n",
    "    display(tree.render(f\"%%inline\", tree_style=ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cognate detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.163795Z",
     "start_time": "2019-08-08T12:37:26.572Z"
    }
   },
   "outputs": [],
   "source": [
    "from cognatedetection import cd\n",
    "\n",
    "### TODO: this part from previous code should not be executed:\n",
    "# print(\"Filter val/test sets on cognates.\")\n",
    "# Use only cognate pairs for validation and test\n",
    "# val[lang_pair] = val[lang_pair].filter_cognates()\n",
    "# test[lang_pair] = test[lang_pair].filter_cognates()\n",
    "# print(\"Val/test sizes after cognate filtering: \" + str(val[lang_pair].get_size()) + \"|\" + str(test[lang_pair].get_size()))\n",
    "\n",
    "\n",
    "print(\"Performing WP cognate detection using clustering...\")\n",
    "results_table = cd.cognate_detection_cluster(lang_pairs, config[\"results_dir\"], options, use_distance=\"prediction\")\n",
    "print(results_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
