{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word prediction in historical linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-01 16:05:47,501 [INFO] Successfully changed parameters.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2e02a869c687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_program\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mintersection_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaselines_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_corpus\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/prediction-histling/dataset/data.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(corpus_name)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Set variables for train corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_corpus\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"northeuralex\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0minput_path_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/northeuralex-0.9-lingpy.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_corpus\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ielex\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_corpus\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ielex-corr\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "from util import init\n",
    "from dataset import data\n",
    "from util.config import config\n",
    "\n",
    "options = init.initialize_program()\n",
    "intersection_path, distances_path, baselines_path, lang_pairs = data.load_data(config[\"train_corpus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show number of cognates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Show number of cognates per language\")\n",
    "cog_per_lang, cliques = data.compute_n_cognates(lang_pairs, tsv_cognates_path_train, langs=config[\"languages\"], cognates_threshold=100)\n",
    "print(\"Cognates per language: \" + str(cog_per_lang))\n",
    "print(\"Number of cliques: \" + str(cliques))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform word prediction algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang_pair in lang_pairs:\n",
    "    lang_a, lang_b = lang_pair\n",
    "    context_vectors_path[lang_pair] = utility.create_path(config[\"results_dir\"], options, prefix=\"context_vectors_\", lang_a=lang_a, lang_b=lang_b)\n",
    "    # Create export path, containing all options\n",
    "    # This is used to output a prediction results file, which can then be used for visualization and cognate detection\n",
    "    results_path[lang_pair] = utility.get_results_path(lang_a, lang_b, config[\"results_dir\"], options)\n",
    "    subs_st_path[lang_pair] = utility.create_path(config[\"results_dir\"], options, prefix=\"subs_st_\", lang_a=lang_a, lang_b=lang_b)\n",
    "    subs_sp_path[lang_pair] = utility.create_path(config[\"results_dir\"], options, prefix=\"subs_sp_\", lang_a=lang_a, lang_b=lang_b)\n",
    "\n",
    "    if config[\"prediction\"] or config[\"baseline\"]:\n",
    "        # If data in pickle, load pickle\n",
    "        data_pickle = results_path[lang_pair] + \"-data.p\"\n",
    "        if os.path.exists(data_pickle):\n",
    "            with open(data_pickle, \"rb\") as f:\n",
    "                print(\"Loading train/val/test sets from pickle, nothing generated.\")\n",
    "                train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], max_len[lang_pair[0]], max_len[lang_pair[1]], voc_size[0], voc_size[1] = pickle.load(f)\n",
    "        else:\n",
    "            # For phylogenetic word prediction, we have a language-independent feature matrix\n",
    "            if not config[\"phyl\"]:\n",
    "                print(\"Create feature matrix for this specific language pair.\")\n",
    "                features, max_len[lang_pair[0]], max_len[lang_pair[1]], voc_size[0], voc_size[1] = data.get_corpus_info([tsv_cognates_path_train + \".tsv\", tsv_cognates_path_valtest + \".tsv\"], lang_pair=lang_pair, input_encoding=config[\"input_encoding\"], output_encoding=config[\"output_encoding\"], feature_matrix_phon=feature_matrix_phon)\n",
    "                conversion_key[lang_pair] = data.create_conversion_key(features)\n",
    "            else:\n",
    "                # In phylogenetic mode, we created one feature matrix for all languages\n",
    "                conversion_key[lang_pair] = conversion_key_general\n",
    "                voc_size = voc_size_general\n",
    "\n",
    "            print(\"Convert training corpus TSV file to data matrix\")\n",
    "            dataset_train, train_mean, train_std = data.create_data_matrix(tsv_path=tsv_cognates_path_train + \".tsv\", lang_pair=(lang_a, lang_b), features=features, max_len=(max_len[lang_pair[0]], max_len[lang_pair[1]]), voc_size=voc_size, batch_size=config[\"batch_size\"], mean_subtraction=config[\"mean_subtraction\"], feature_standardization=not config[\"no_standardization\"], excluded_concepts=excluded_concepts_training, cognate_detection=config[\"cognate_detection\"])\n",
    "\n",
    "            print(\"Convert val/test corpus TSV file to data matrix\")\n",
    "            dataset_valtest, _, _ = data.create_data_matrix(tsv_path=tsv_cognates_path_valtest + \".tsv\", lang_pair=(lang_a, lang_b), features=features, max_len=(max_len[lang_pair[0]], max_len[lang_pair[1]]), voc_size=voc_size, batch_size=config[\"batch_size\"], mean_subtraction=config[\"mean_subtraction\"], feature_standardization=not config[\"no_standardization\"], cognate_detection=config[\"cognate_detection\"], valtest=True, train_mean=train_mean, train_std=train_std)\n",
    "\n",
    "            t_set_size = dataset_train.get_size()\n",
    "            vt_set_size = dataset_valtest.get_size()\n",
    "\n",
    "            if config[\"valtest_corpus\"] == config[\"train_corpus\"]:\n",
    "                # If train and valtest corpus the same, divide one corpus into parts\n",
    "                assert t_set_size == vt_set_size\n",
    "                n_train, n_val, n_test = dataset_train.compute_subset_sizes(t_set_size)\n",
    "            else:\n",
    "                # If train and valtest corpus different, use full train corpus as train and\n",
    "                # full valtest corpus for validation and testing\n",
    "                # TODO: In fact this is not needed, we can directly take set size.\n",
    "                n_train, _, _ = dataset_train.compute_subset_sizes(t_set_size, only_train=True)\n",
    "                _, n_val, n_test = dataset_valtest.compute_subset_sizes(vt_set_size, only_valtest=True)\n",
    "\n",
    "            print(\"Divide into training, validation and test set.\")\n",
    "            # Even if train and valtest corpus are the same, we do this separately,\n",
    "            # because valtest corpus is filtered on cognates and train corpus is not\n",
    "            # Use train corpus only for train set\n",
    "            train[lang_pair], _, _ = dataset_train.divide_subsets(n_train, 0, 0)\n",
    "            # Use val/test corpus for validation and test set\n",
    "            _, val[lang_pair], test[lang_pair] = dataset_valtest.divide_subsets(0, n_val, n_test)\n",
    "\n",
    "            if not config[\"cognate_detection\"]:\n",
    "                print(\"Filter val/test sets on cognates.\")\n",
    "                # Use only cognate pairs for validation and test\n",
    "                val[lang_pair] = val[lang_pair].filter_cognates()\n",
    "                test[lang_pair] = test[lang_pair].filter_cognates()\n",
    "                print(\"Val/test sizes after cognate filtering: \" + str(val[lang_pair].get_size()) + \"|\" + str(test[lang_pair].get_size()))\n",
    "\n",
    "            # Pickle train/val/test/sets\n",
    "            with open(data_pickle, \"wb\") as f:\n",
    "                pickle.dump((train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], max_len[lang_pair[0]], max_len[lang_pair[1]], voc_size[0], voc_size[1]), f)\n",
    "\n",
    "    if config[\"prediction\"] and not config[\"seq\"] and not config[\"phyl\"]:\n",
    "        print(\"Performing word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "        prediction.word_prediction(lang_a, lang_b, (max_len[lang_pair[0]], max_len[lang_pair[1]]), train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], voc_size, results_path[lang_pair], distances_path + \".txt\", context_vectors_path[lang_pair] + \".p\", config[\"output_encoding\"], config)\n",
    "    if config[\"prediction\"] and config[\"seq\"] and not config[\"phyl\"]:\n",
    "        print(\"Performing SeqModel word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "        prediction.word_prediction_seq(lang_a, lang_b, train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], results_path[lang_pair], distances_path + \".txt\", config)\n",
    "    if config[\"baseline\"] and config[\"input_type\"] == \"asjp\":\n",
    "        print(\"Performing baseline results for pair(\" + lang_a + \", \" + lang_b + \")\")\n",
    "        sounds = (list(features[0].index), list(features[1].index))\n",
    "        training_frame = train[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "        testing_frame = test[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "        baseline.compute_baseline(lang_a, lang_b, sounds, training_frame, testing_frame, baselines_path + \".txt\")\n",
    "    if config[\"visualize\"]:\n",
    "        print(\"Inferring sound correspondences...\")\n",
    "        visualize.show_output_substitutions(results_path[lang_pair], subs_st_path[lang_pair], subs_sp_path[lang_pair])\n",
    "    if config[\"visualize_weights\"]:\n",
    "        visualize.visualize_weights(context_vectors_path[lang_pair], lang_pair, config[\"input_encoding\"], config[\"output_encoding\"], config[\"results_dir\"], sample=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lang_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-745fb079e520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvisualize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlang_pair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlang_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Create embedding for first languages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0memb_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtsv_cognates_path_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtsv_cognates_path_valtest\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tsv\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lang_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "from dataset import data\n",
    "from visualize import visualize\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    # Create embedding for first languages\n",
    "    emb_matrix = data.create_embedding(lang_pair[0], [tsv_cognates_path_train + \".tsv\", tsv_cognates_path_valtest + \".tsv\"])\n",
    "    visualize.visualize_encoding(emb_matrix, feature_matrix_phon, lang_pair, config[\"results_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phylogenetic tree reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering based on word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Cluster based on word prediction distances\n",
    "print(\"WP TREE:\\n\")\n",
    "cluster.cluster_languages(lang_pairs, distances_path, output_path=distances_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Source prediction baseline\n",
    "print(\"\\nSOURCE BASELINE TREE\")\n",
    "cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_source\", distance_col=2)\n",
    "# PMI-based baseline\n",
    "print(\"\\nPMI BASELINE TREE\")\n",
    "cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_pmi\", distance_col=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cognate detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing WP cognate detection using clustering...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-170c7a34631a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Performing WP cognate detection using clustering...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcognate_detection_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"results_dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cd' is not defined"
     ]
    }
   ],
   "source": [
    "from cognatedetection import cd\n",
    "\n",
    "print(\"Performing WP cognate detection using clustering...\")\n",
    "results_table = cd.cognate_detection_cluster(lang_pairs, config[\"results_dir\"], options, use_distance=\"prediction\")\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phylogenetic word prediction (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import utility\n",
    "from prediction import prediction\n",
    "\n",
    "config[\"export_weights\"] = False  # Turn off export of weights\n",
    "tree_string = \"((nld,deu),eng)\"  # unused at the moment\n",
    "if len(config[\"languages\"]) >= 3:\n",
    "    results_path_proto = utility.create_path(config[\"results_dir\"], options, prefix=\"proto_\")  # lang-pair independent path\n",
    "    prediction.word_prediction_phyl(config[\"languages\"], lang_pairs, tree_string, max_len, train, val, test, conversion_key_general, voc_size, results_path, results_path_proto, distances_path + \".txt\", context_vectors_path, plot_path_phyl, config[\"output_encoding\"], config)\n",
    "else:\n",
    "    print(\"Please supply 3 languages, the first 2 being more closely related than the last.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
