{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word prediction in historical linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T15:07:51.388051Z",
     "start_time": "2019-08-08T15:07:14.853998Z"
    }
   },
   "outputs": [],
   "source": [
    "from util import init\n",
    "from dataset import data\n",
    "from util.config import config\n",
    "from prediction import prediction\n",
    "import sys\n",
    "\n",
    "lang_family_dict = {\n",
    "\"slav\": [\"ces\", \"bul\", \"rus\", \"bel\", \"ukr\", \"pol\", \"slk\", \"slv\", \"hrv\"],\n",
    "\"ger\": [\"swe\", \"isl\", \"eng\", \"nld\", \"deu\", \"dan\", \"nor\"]\n",
    "}\n",
    "\n",
    "# As user, you can either set separate languages or a language family\n",
    "languages = [\"nld\",\"deu\"]\n",
    "lang_family = None\n",
    "\n",
    "\n",
    "if lang_family:\n",
    "    languages = lang_family_dict[lang_family]\n",
    "\n",
    "options, distances_path, baselines_path = init.initialize_program()\n",
    "results_path, output_path_cognates_train, output_path_cognates_valtest, context_vectors_path, lang_pairs, train, val, test, max_len, conversion_key, voc_size = data.load_data(train_corpus=\"northeuralex\",\n",
    "                                                                                               valtest_corpus=\"northeuralex\",\n",
    "                                                                                               languages=languages,  \n",
    "                                                                                               input_type=\"asjp\", \n",
    "                                                                                               options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Show number of cognates in training data\n",
    "Show the number of cognate word pairs per language pair in the training data, and calculate cliques of languages with a minimum of 100 shared cognates. These cliques can later be used, to have a group of languages with a large shared number of cognates, to perform prediction on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.156995Z",
     "start_time": "2019-08-08T12:36:42.898Z"
    }
   },
   "outputs": [],
   "source": [
    "cog_per_lang, cliques = data.compute_n_cognates(lang_pairs, output_path_cognates_train, langs=languages, cognates_threshold=100)\n",
    "print(\"Cognates per language: \")\n",
    "print((cog_per_lang))\n",
    "print(\"Cliques: \")\n",
    "for c in cliques:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word prediction using structured perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.158867Z",
     "start_time": "2019-08-08T12:37:12.451Z"
    }
   },
   "outputs": [],
   "source": [
    "for lang_pair in lang_pairs:\n",
    "    print(\"Performing structured perceptron word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "    prediction.word_prediction_seq(lang_a, lang_b, train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], results_path[lang_pair], distances_path + \".txt\", config)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word prediction using encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.157919Z",
     "start_time": "2019-08-08T12:37:07.979Z"
    }
   },
   "outputs": [],
   "source": [
    "for lang_pair in lang_pairs:\n",
    "    lang_a,lang_b = lang_pair\n",
    "    print(\"Performing RNN word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "    prediction.word_prediction(lang_a, lang_b, (max_len[lang_a], max_len[lang_b]), train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], voc_size, results_path[lang_pair], distances_path + \".txt\", context_vectors_path[lang_pair] + \".p\", config[\"output_encoding\"], config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.159715Z",
     "start_time": "2019-08-08T12:37:14.818Z"
    }
   },
   "outputs": [],
   "source": [
    "# Only ASJP\n",
    "for lang_pair in lang_pairs:\n",
    "    sounds = (list(features[0].index), list(features[1].index))\n",
    "    training_frame = train[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "    testing_frame = test[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "    baseline.compute_baseline(lang_a, lang_b, sounds, training_frame, testing_frame, baselines_path + \".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.160590Z",
     "start_time": "2019-08-08T12:37:17.651Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import data\n",
    "from visualize import visualize\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    # Create embedding every first language in language pair\n",
    "    emb_matrix = data.create_embedding(lang_pair[0], [output_path_cognates_train, output_path_cognates_valtest])\n",
    "    visualize.visualize_encoding(emb_matrix, feature_matrix_phon, lang_pair, config[\"results_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify sound correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.161412Z",
     "start_time": "2019-08-08T12:37:20.235Z"
    }
   },
   "outputs": [],
   "source": [
    "from visualize import visualize\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    visualize.show_output_substitutions(results_path[lang_pair], subs_st_path[lang_pair], subs_sp_path[lang_pair])\n",
    "    visualize.visualize_weights(context_vectors_path[lang_pair], lang_pair, config[\"input_encoding\"], config[\"output_encoding\"], config[\"results_dir\"], sample=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phylogenetic tree reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering based on word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.162205Z",
     "start_time": "2019-08-08T12:37:22.524Z"
    }
   },
   "outputs": [],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Cluster based on word prediction distances\n",
    "print(\"WP TREE:\\n\")\n",
    "cluster.cluster_languages(lang_pairs, distances_path, output_path=distances_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.162967Z",
     "start_time": "2019-08-08T12:37:24.347Z"
    }
   },
   "outputs": [],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Source prediction baseline\n",
    "print(\"\\nSOURCE BASELINE TREE\")\n",
    "cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_source\", distance_col=2)\n",
    "# PMI-based baseline\n",
    "print(\"\\nPMI BASELINE TREE\")\n",
    "cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_pmi\", distance_col=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cognate detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.163795Z",
     "start_time": "2019-08-08T12:37:26.572Z"
    }
   },
   "outputs": [],
   "source": [
    "from cognatedetection import cd\n",
    "\n",
    "### TODO: this part from previous code should not be executed:\n",
    "# print(\"Filter val/test sets on cognates.\")\n",
    "# Use only cognate pairs for validation and test\n",
    "# val[lang_pair] = val[lang_pair].filter_cognates()\n",
    "# test[lang_pair] = test[lang_pair].filter_cognates()\n",
    "# print(\"Val/test sizes after cognate filtering: \" + str(val[lang_pair].get_size()) + \"|\" + str(test[lang_pair].get_size()))\n",
    "\n",
    "\n",
    "print(\"Performing WP cognate detection using clustering...\")\n",
    "results_table = cd.cognate_detection_cluster(lang_pairs, config[\"results_dir\"], options, use_distance=\"prediction\")\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phylogenetic word prediction (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.164558Z",
     "start_time": "2019-08-08T12:37:28.854Z"
    }
   },
   "outputs": [],
   "source": [
    "from util import utility\n",
    "from prediction import prediction\n",
    "\n",
    "\n",
    "# In phylogenetic mode, we created one feature matrix for all languages\n",
    "for lang_pair in lang_pairs:\n",
    "    conversion_key[lang_pair] = conversion_key_general\n",
    "\n",
    "voc_size = voc_size_general\n",
    "# For phylogenetic word prediction, create one feature matrix for all languages\n",
    "print(\"Create feature matrix for all language pairs.\")\n",
    "used_tokens = [[], []]\n",
    "tokens_set = [[], []]\n",
    "for lang_pair in lang_pairs:\n",
    "    # For phylogenetic word prediction, create one feature matrix for all languages\n",
    "    features_lp[lang_pair], max_len[lang_pair[0]], max_len[lang_pair[1]], _, _ = data.get_corpus_info([tsv_cognates_path_train + \".tsv\", tsv_cognates_path_valtest + \".tsv\"], lang_pair=lang_pair, input_encoding=config[\"input_encoding\"], output_encoding=config[\"output_encoding\"], feature_matrix_phon=feature_matrix_phon)\n",
    "    used_tokens[0] += list(features_lp[lang_pair][0].index)\n",
    "    used_tokens[1] += list(features_lp[lang_pair][1].index)\n",
    "\n",
    "tokens_set[0] = list(set(used_tokens[0]))\n",
    "tokens_set[1] = list(set(used_tokens[1]))\n",
    "if config[\"input_encoding\"] == \"character\":\n",
    "    features[0] = data.create_one_hot_matrix(tokens_set[0])\n",
    "elif config[\"input_encoding\"] == \"phonetic\":\n",
    "    features[0] = feature_matrix_phon.loc[tokens_set[0]]\n",
    "else:\n",
    "    print(\"Embedding encoding not possible in phylogenetic tree prediction.\")\n",
    "    return\n",
    "# Output encoding is always character\n",
    "features[1] = data.create_one_hot_matrix(tokens_set[1])\n",
    "voc_size_general[0] = features[0].shape[1]\n",
    "voc_size_general[1] = features[1].shape[1]\n",
    "conversion_key_general = data.create_conversion_key(features)\n",
    "plot_path_phyl = utility.create_path(config[\"results_dir\"], options, prefix=\"plot_\")\n",
    "\n",
    "config[\"export_weights\"] = False  # Turn off export of weights\n",
    "tree_string = \"((nld,deu),eng)\"  # unused at the moment\n",
    "if len(config[\"languages\"]) >= 3:\n",
    "    results_path_proto = utility.create_path(config[\"results_dir\"], options, prefix=\"proto_\")  # lang-pair independent path\n",
    "    prediction.word_prediction_phyl(config[\"languages\"], lang_pairs, tree_string, max_len, train, val, test, conversion_key_general, voc_size, results_path, results_path_proto, distances_path + \".txt\", context_vectors_path, plot_path_phyl, config[\"output_encoding\"], config)\n",
    "else:\n",
    "    print(\"Please supply 3 languages, the first 2 being more closely related than the last.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ph-env",
   "language": "python",
   "name": "ph-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
