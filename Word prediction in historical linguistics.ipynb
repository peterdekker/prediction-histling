{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word prediction in historical linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application and data loading\n",
    "Give a list of languages (from [NorthEuraLex](http://northeuralex.org/languages)) to take under consideration in the `languages` variable. Or set `lang_family` to a language family: `slav` or `ger`. If `lang_family` has been set (not `None`), `languages` will be disregarded.\n",
    "\n",
    "There is also a variable `cognate_detection`, which is set to `False` by default. This allows multiple forms per concept, which leads to a larger data set. If you would like to be able to perform cognate detection based on prediction results (a later step in this notebook), set this variable to `True`, which restricts the data set to one form per concept.\n",
    "\n",
    "Other, more specific, options can be set in the file `util/config.py`, before this cell is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T15:07:51.388051Z",
     "start_time": "2019-08-08T15:07:14.853998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing program...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-09 18:43:57,169 [INFO] Successfully changed parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading phonetic feature matrix...\n",
      "Generating all language pairs...\n",
      "Training corpus:\n",
      " - Loading dataset and performing necessary conversion/tokenization.\n",
      "Using existing wordlist file, nothing is generated.\n",
      " - Detect cognates in entire dataset using LexStat.\n",
      "Using existing cognates file output/northeuralex-asjp-cognates.tsv, nothing is generated.\n",
      "Train corpus is valtest corpus.\n",
      "Loading train/val/test sets from pickle, nothing generated...\n",
      "Loading train/val/test sets from pickle, nothing generated...\n",
      "Loading train/val/test sets from pickle, nothing generated...\n",
      "Loading train/val/test sets from pickle, nothing generated...\n",
      "Loading train/val/test sets from pickle, nothing generated...\n",
      "Loading train/val/test sets from pickle, nothing generated...\n",
      "Loading train/val/test sets from pickle, nothing generated...\n",
      "Loading train/val/test sets from pickle, nothing generated...\n",
      "Loading train/val/test sets from pickle, nothing generated...\n",
      "Loading train/val/test sets from pickle, nothing generated...\n",
      "Loading train/val/test sets from pickle, nothing generated...\n",
      "Loading train/val/test sets from pickle, nothing generated...\n",
      "Done loading data.\n"
     ]
    }
   ],
   "source": [
    "from util import init\n",
    "from dataset import data\n",
    "from util.config import config\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "lang_family_dict = {\n",
    "\"slav\": [\"ces\", \"bul\", \"rus\", \"bel\", \"ukr\", \"pol\", \"slk\", \"slv\", \"hrv\"],\n",
    "\"ger\": [\"swe\", \"isl\", \"eng\", \"nld\", \"deu\", \"dan\", \"nor\"]\n",
    "}\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# VARIABLES TO BE SET BY USERv\n",
    "languages = [\"nld\",\"deu\", \"swe\", \"isl\"]\n",
    "lang_family = None\n",
    "cognate_detection = False\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "\n",
    "if lang_family:\n",
    "    languages = lang_family_dict[lang_family]\n",
    "\n",
    "    \n",
    "options, distances_path, baselines_path = init.initialize_program(cognate_detection)\n",
    "(results_path, output_path_cognates_train, output_path_cognates_valtest,\n",
    "context_vectors_path, subs_sp_path, subs_st_path, lang_pairs, train, val, test, max_len, \n",
    "conversion_key, voc_size, feature_matrix_phon) = data.load_data(train_corpus=\"northeuralex\",\n",
    "                                                               valtest_corpus=\"northeuralex\",\n",
    "                                                               languages=languages,  \n",
    "                                                               input_type=\"asjp\", \n",
    "                                                               options=options,\n",
    "                                                               cognate_detection=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Enable weight visualization\n",
    "Run the following cell, to be able to export and visualize the weights of the network. Do this only for diagnostic/visualization purposes, because the batch size will be set to 1, which is not optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"export_weights\"]=True\n",
    "config[\"batch_size\"]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize encoding\n",
    "Visualize the representation of phonemes in the embedding encoding, as PCA and as hierarchically clustered tree. Compare them to the phonetic feature matrix from Brown (2008)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.160590Z",
     "start_time": "2019-08-08T12:37:17.651Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import data\n",
    "from visualize import visualize\n",
    "from tree import cluster\n",
    "\n",
    "tree_style = config[\"ete_tree_style\"]\n",
    "\n",
    "print(\"Phonetic matrix from Brown (2008):\")\n",
    "# Perform PCA on phonetic feature matrix from Brown (2008)\n",
    "phon_matrix_red, phon_phonemes = visualize.dim_reduction(feature_matrix_phon)\n",
    "# Visualize phonetic feature PCA using plot\n",
    "visualize.visualize_encoding(phon_matrix_red, phon_phonemes, \"phonetic-pca\")\n",
    "# Hierarchically cluster distances between phonemes in phonetic feature matrix\n",
    "tree = cluster.cluster_phonemes_encoding(feature_matrix_phon, phon_phonemes, \"phonetic\")\n",
    "\n",
    "display(tree.render(\"%%inline\", tree_style=tree_style))\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    lang_a = lang_pair[0]\n",
    "    print(f\"Embedding for {lang_a}:\")\n",
    "    # Create embedding for every first language in language pair\n",
    "    emb_matrix = data.create_embedding(lang_a, [output_path_cognates_train, output_path_cognates_valtest])\n",
    "    # Perform PCA on embedding matrix\n",
    "    emb_matrix_red, emb_phonemes = visualize.dim_reduction(emb_matrix)\n",
    "    # Visualize embedding PCA using plot\n",
    "    visualize.visualize_encoding(emb_matrix_red, emb_phonemes, f\"embedding-{lang_pair[0]}-pca\")\n",
    "    \n",
    "    # Hierarchically cluster distances between phonemes in embedding matrix\n",
    "    tree = cluster.cluster_phonemes_encoding(emb_matrix, emb_phonemes, f\"embedding-{lang_pair[0]}\")\n",
    "    display(tree.render(\"%%inline\", tree_style=tree_style))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close-up: constrast occurrences of phonemes in data\n",
    "Interesting patterns in the phoneme encoding visualizations, can be looked up in the data. In the Dutch embedding encoding, we saw that *t* and *d*, closely related phonemes, are quite remote in the embedding space. How do the words with *t* and *d*, on which the embedding encoding is based, look in Dutch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lang = \"nld\"\n",
    "# Read in TSV file with data\n",
    "df = pd.read_csv(output_path_cognates_train, sep=\"\\t\", engine=\"python\", skipfooter=3, index_col=False)\n",
    "df_lang = df[df[\"DOCULECT\"] == lang]\n",
    "\n",
    "for phoneme in [\"d\", \"t\"]:\n",
    "    print(phoneme)\n",
    "    words_with_phoneme = df_lang[df_lang[\"ASJP\"].str.contains(phoneme)]\n",
    "    total = len(words_with_phoneme)\n",
    "    print(f\"Total number of occurrences: {total}\")\n",
    "    \n",
    "    # Compute locations of phonemes in word\n",
    "    locations = df_lang[\"ASJP\"].str.find(phoneme)\n",
    "    locations = locations[locations != -1]\n",
    "    # Compute relative frequencies\n",
    "    locations_relfreq = locations.value_counts(normalize=True)\n",
    "    print(\"Relative frequencies of locations:\")\n",
    "    print(locations_relfreq)\n",
    "    # Look up words with most frequent location\n",
    "    most_freq_location = int(locations.mode())\n",
    "    words_in_most_freq_loc = words_with_phoneme[words_with_phoneme[\"ASJP\"].str.find(phoneme) == most_freq_location]\n",
    "    print(f\"Words with {phoneme} where {phoneme} has most frequent location in word ({most_freq_location}):\")\n",
    "    print(words_in_most_freq_loc)\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "#words_list = list(df_lang[\"TOKENS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Show number of cognates in training data\n",
    "Show the number of cognate word pairs per language pair in the training data, and calculate cliques of languages with a minimum of 100 shared cognates. These cliques can later be used, to have a group of languages with a large shared number of cognates, to perform prediction on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.156995Z",
     "start_time": "2019-08-08T12:36:42.898Z"
    }
   },
   "outputs": [],
   "source": [
    "cog_per_lang, cliques = data.compute_n_cognates(lang_pairs, output_path_cognates_train, langs=languages, cognates_threshold=100)\n",
    "print(\"Cognates per language: \")\n",
    "print((cog_per_lang))\n",
    "print(\"Cliques: \")\n",
    "for c in cliques:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word prediction using structured perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.158867Z",
     "start_time": "2019-08-08T12:37:12.451Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2eda3bcd4a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlang_pair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlang_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlang_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang_pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Performing structured perceptron word prediction for pair (\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlang_a\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlang_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\")\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/prediction-histling/prediction/prediction.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncoderDecoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeqModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeqModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/prediction-histling/models/SeqModel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mseqlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperceptron\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStructuredPerceptron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/seqlearn/perceptron.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSequenceClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m from ._utils import (atleast2d_or_csr, check_random_state, count_trans,\n\u001b[1;32m     14\u001b[0m                      make_trans_matrix, safe_add, safe_sparse_dot)\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/seqlearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_decode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDECODERS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0matleast2d_or_csr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/seqlearn/_decode/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbestfirst\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbestfirst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mviterbi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mviterbi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mseqlearn/_decode/bestfirst.pyx\u001b[0m in \u001b[0;36minit seqlearn._decode.bestfirst (seqlearn/_decode/bestfirst.c:4928)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from prediction import prediction\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    lang_a, lang_b = lang_pair\n",
    "    print(\"Performing structured perceptron word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "    prediction.word_prediction_seq(lang_a, lang_b, train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], results_path[lang_pair], distances_path + \".txt\", config)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word prediction using encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.157919Z",
     "start_time": "2019-08-08T12:37:07.979Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'bestfirst'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8c1f25e3d38c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlang_pair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlang_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlang_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlang_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang_pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{voc_size[lang_a]} {voc_size[lang_b]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Performing RNN word prediction for pair (\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlang_a\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlang_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\")\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/prediction-histling/prediction/prediction.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncoderDecoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeqModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeqModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/prediction-histling/models/SeqModel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mseqlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperceptron\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStructuredPerceptron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/seqlearn/perceptron.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSequenceClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m from ._utils import (atleast2d_or_csr, check_random_state, count_trans,\n\u001b[1;32m     14\u001b[0m                      make_trans_matrix, safe_add, safe_sparse_dot)\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/seqlearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_decode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDECODERS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0matleast2d_or_csr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/seqlearn/_decode/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbestfirst\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbestfirst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mviterbi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mviterbi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'bestfirst'"
     ]
    }
   ],
   "source": [
    "from prediction import prediction\n",
    "for lang_pair in lang_pairs:\n",
    "    lang_a,lang_b = lang_pair\n",
    "    print(f\"{voc_size[lang_a]} {voc_size[lang_b]}\")\n",
    "    print(\"Performing RNN word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "    prediction.word_prediction_rnn(lang_a, lang_b, (max_len[lang_a], max_len[lang_b]), train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], (voc_size[lang_a],voc_size[lang_b]), results_path[lang_pair], distances_path + \".txt\", context_vectors_path[lang_pair] + \".p\", config[\"output_encoding\"], config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.159715Z",
     "start_time": "2019-08-08T12:37:14.818Z"
    }
   },
   "outputs": [],
   "source": [
    "from models import baseline\n",
    "\n",
    "# Only ASJP\n",
    "for lang_pair in lang_pairs:\n",
    "    lang_a,lang_b = lang_pair\n",
    "    conv = conversion_key[lang_pair]\n",
    "    sounds = (list(conv[0].values()), list(conv[1].values()))\n",
    "    training_frame = train[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "    testing_frame = test[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "    baseline.compute_baseline(lang_a, lang_b, sounds, training_frame, testing_frame, baselines_path + \".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify sound correspondences\n",
    "##### Based on output substitutions\n",
    "This shows the substitutions table in the notebook, and outputs it as LaTeX table to `RESULTS_DIR/subs.tex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.161412Z",
     "start_time": "2019-08-08T12:37:20.235Z"
    }
   },
   "outputs": [],
   "source": [
    "from visualize import visualize\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    visualize.show_output_substitutions(results_path[lang_pair], subs_st_path[lang_pair], subs_sp_path[lang_pair])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on context vector weights (only when weight exported activated)\n",
    "This works when word prediction with encoder-decoder has been run, and weight export has been activated at the top of this notebook. Executing this command shows the distances between input, output and context vectors. Furthermore, it generates PCA plots of the input, output and context vectors and generates LaTeX files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import visualize\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    visualize.visualize_weights(context_vectors_path[lang_pair], lang_pair, config[\"input_encoding\"], config[\"output_encoding\"], config[\"results_dir\"], sample=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phylogenetic tree reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering based on word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.162205Z",
     "start_time": "2019-08-08T12:37:22.524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('deu', 'isl'), 0.6009206349206349), (('deu', 'nld'), 0.3073015873015873), (('deu', 'swe'), 0.5374313900014834), (('isl', 'deu'), 0.6276031746031746), (('isl', 'nld'), 0.6811265969802556), (('isl', 'swe'), 0.5258455718736093), (('nld', 'deu'), 0.3689415728304617), (('nld', 'isl'), 0.6395751633986929), (('nld', 'swe'), 0.5006974506974506), (('swe', 'deu'), 0.5249801081576781), (('swe', 'isl'), 0.4553293279928794), (('swe', 'nld'), 0.4621704787749564)]\n",
      " - Creating tree using UPGMA, saving to .nw and .pdf\n",
      " - Creating tree using Neighbour joining, saving to .nw and .pdf\n",
      "Mean distance of all lang pairs: 0.5193\n",
      "nld,deu,0.6456795341410726\n",
      "nld,swe,0.7397985347985347\n",
      "nld,isl,0.7881944444444444\n",
      "deu,nld,0.6477014652014652\n",
      "deu,swe,0.6923690476190475\n",
      "deu,isl,0.758015873015873\n",
      "swe,nld,0.7656188256188255\n",
      "swe,deu,0.7095699855699855\n",
      "swe,isl,0.6942420634920634\n",
      "isl,nld,0.7700446428571428\n",
      "isl,deu,0.7768197278911564\n",
      "isl,swe,0.6995793650793651\n",
      "nld,deu,0.3689415728304617\n",
      "nld,swe,0.5006974506974506\n",
      "nld,isl,0.6395751633986929\n",
      "deu,nld,0.3073015873015873\n",
      "deu,swe,0.5374313900014834\n",
      "deu,isl,0.6009206349206349\n",
      "swe,nld,0.4621704787749564\n",
      "swe,deu,0.5249801081576781\n",
      "swe,isl,0.4553293279928794\n",
      "isl,nld,0.6811265969802556\n",
      "isl,deu,0.6276031746031746\n",
      "isl,swe,0.5258455718736093\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Cluster based on word prediction distances\n",
    "trees = cluster.cluster_languages(lang_pairs, distances_path, output_path=distances_path)\n",
    "\n",
    "# Show trees in notebook\n",
    "for tree in trees:\n",
    "    tree.render(\"%%inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering based on baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.162967Z",
     "start_time": "2019-08-08T12:37:24.347Z"
    }
   },
   "outputs": [],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Source prediction baseline\n",
    "print(\"\\nSOURCE BASELINE TREE\")\n",
    "cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_source\", distance_col=2)\n",
    "# PMI-based baseline\n",
    "print(\"\\nPMI BASELINE TREE\")\n",
    "cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_pmi\", distance_col=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Draw tree from existing newick string (no distance calculcation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Improvised code to re-generate trees\n",
    "from ete3 import Tree, TreeStyle, NodeStyle, TextFace\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "newick_string1 = \"((bul:0.15,(slv:0.11,hrv:0.11):0.04):0.04,((rus:0.11,(bel:0.1,ukr:0.1):0.01):0.07,(pol:0.15,(ces:0.08,slk:0.08):0.07):0.03):0.01);\"\n",
    "newick_string2 = \"(((bel:0.08,ukr:0.12):0.01,rus:0.12):0.03,(((slv:0.1,hrv:0.11):0.01,bul:0.18):0.07,(pol:0.17,(ces:0.1,slk:0.07):0.05):0.04):0.03);\"\n",
    "newick_string3 = \"((bul:0.29,((ces:0.24,slk:0.24):0.04,(slv:0.25,hrv:0.25):0.03):0.01):0.01,(pol:0.28,(rus:0.24,(bel:0.22,ukr:0.22):0.02):0.04):0.01);\"\n",
    "newick_string4 = \"((bel,rus,ukr),((hrv,slv),bul),((ces,slk),pol));\"\n",
    "\n",
    "ts = TreeStyle()\n",
    "ts.show_scale = False\n",
    "ts.show_leaf_name = False\n",
    "ts.force_topology = False\n",
    "ts.show_border = False\n",
    "ts.margin_top = ts.margin_bottom = ts.margin_right = ts.margin_left = 5\n",
    "ts.scale = 500\n",
    "ts.branch_vertical_margin= 10\n",
    "\n",
    "\n",
    "for i,newick_string in enumerate([newick_string1, newick_string2, newick_string3, newick_string4]):\n",
    "    if i==3: # last newick string without lengths, should be corrected\n",
    "        ts = TreeStyle()\n",
    "        ts.show_scale = False\n",
    "        ts.show_leaf_name = False\n",
    "        ts.force_topology = False\n",
    "        ts.show_border = False\n",
    "        ts.margin_top = ts.margin_bottom = ts.margin_right = ts.margin_left = 5\n",
    "        ts.scale = 50\n",
    "        ts.branch_vertical_margin= 10\n",
    "    # Load newick string into ete3 Tree object\n",
    "    tree = Tree(newick_string)\n",
    "    for node in tree.traverse():\n",
    "        node.set_style(config[\"ete_node_style\"])\n",
    "        if node.is_leaf():\n",
    "            # Add bit of extra space between leaf branch and leaf label\n",
    "            name_face = TextFace(f\" {node.name}\", fgcolor=\"black\", fsize=10)\n",
    "            node.add_face(name_face, column=0, position='branch-right')\n",
    "    print(f\"output/tree{i+1}.pdf\")\n",
    "    tree.render(f\"output/tree{i+1}.pdf\", tree_style=ts)\n",
    "    display(tree.render(f\"%%inline\", tree_style=ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cognate detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:42:48.163795Z",
     "start_time": "2019-08-08T12:37:26.572Z"
    }
   },
   "outputs": [],
   "source": [
    "from cognatedetection import cd\n",
    "\n",
    "### TODO: this part from previous code should not be executed:\n",
    "# print(\"Filter val/test sets on cognates.\")\n",
    "# Use only cognate pairs for validation and test\n",
    "# val[lang_pair] = val[lang_pair].filter_cognates()\n",
    "# test[lang_pair] = test[lang_pair].filter_cognates()\n",
    "# print(\"Val/test sizes after cognate filtering: \" + str(val[lang_pair].get_size()) + \"|\" + str(test[lang_pair].get_size()))\n",
    "\n",
    "\n",
    "print(\"Performing WP cognate detection using clustering...\")\n",
    "results_table = cd.cognate_detection_cluster(lang_pairs, config[\"results_dir\"], options, use_distance=\"prediction\")\n",
    "print(results_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
