{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word prediction in historical linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T14:29:42.751777Z",
     "start_time": "2019-08-07T14:29:42.426529Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-07 20:21:43,204 [INFO] Successfully changed parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating all language pairs...\n",
      "Training corpus:\n",
      " - Convert wordlists to tsv format, and tokenize words.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown file format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-098f42fe4f4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                                                                                \u001b[0mintersection_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintersection_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                                                                                \u001b[0minput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"asjp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                                                                                                options=options)\n\u001b[0m",
      "\u001b[0;32m~/git/prediction-histling/dataset/data.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(train_corpus, valtest_corpus, languages, intersection_path, input_type, options)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training corpus:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" - Convert wordlists to tsv format, and tokenize words.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtsv_path_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintersection_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintersection_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" - Detect cognates in entire dataset using LexStat.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mcd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcognate_detection_lexstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsv_path_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtsv_cognates_path_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/prediction-histling/dataset/data.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(input_path, source, input_type, output_path, intersection_path)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;31m#     df.drop(\"ID\", axis=1, inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown file format.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"ielex\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown file format."
     ]
    }
   ],
   "source": [
    "from util import init\n",
    "from dataset import data\n",
    "from util.config import config\n",
    "\n",
    "# As user, you can either set separate languages or a language family\n",
    "languages = [\"nld\",\"deu\"]\n",
    "lang_family = None\n",
    "\n",
    "lang_family_dict = {\n",
    "\"slav\": [\"ces\", \"bul\", \"rus\", \"bel\", \"ukr\", \"pol\", \"slk\", \"slv\", \"hrv\"],\n",
    "\"ger\": [\"swe\", \"isl\", \"eng\", \"nld\", \"deu\", \"dan\", \"nor\"]\n",
    "}\n",
    "if lang_family:\n",
    "    languages = lang_family_dict[lang_family]\n",
    "\n",
    "options, intersection_path, distances_path, baselines_path = init.initialize_program()\n",
    "results_path, lang_pairs, train, val, test, max_len, conversion_key, voc_size = data.load_data(train_corpus=\"northeuralex\",\n",
    "                                                                                               valtest_corpus=\"northeuralex\",\n",
    "                                                                                               languages=languages, \n",
    "                                                                                               intersection_path=intersection_path, \n",
    "                                                                                               input_type=\"asjp\", \n",
    "                                                                                               options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show number of cognates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Show number of cognates per language\")\n",
    "cog_per_lang, cliques = data.compute_n_cognates(lang_pairs, tsv_cognates_path_train, langs=config[\"languages\"], cognates_threshold=100)\n",
    "print(\"Cognates per language: \" + str(cog_per_lang))\n",
    "print(\"Number of cliques: \" + str(cliques))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform word prediction algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang_pair in lang_pairs:\n",
    "    print(\"Performing RNN word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "    prediction.word_prediction(lang_a, lang_b, (max_len[lang_pair[0]], max_len[lang_pair[1]]), train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], voc_size, results_path[lang_pair], distances_path + \".txt\", context_vectors_path[lang_pair] + \".p\", config[\"output_encoding\"], config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word prediction using structured perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang_pair in lang_pairs:\n",
    "    print(\"Performing structured perceptron word prediction for pair (\" + lang_a + \", \" + lang_b + \")\")\n",
    "    prediction.word_prediction_seq(lang_a, lang_b, train[lang_pair], val[lang_pair], test[lang_pair], conversion_key[lang_pair], results_path[lang_pair], distances_path + \".txt\", config)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only ASJP\n",
    "for lang_pair in lang_pairs:\n",
    "    sounds = (list(features[0].index), list(features[1].index))\n",
    "    training_frame = train[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "    testing_frame = test[lang_pair].get_dataframe(conversion_key[lang_pair], config[\"input_encoding\"], config[\"output_encoding\"])\n",
    "    baseline.compute_baseline(lang_a, lang_b, sounds, training_frame, testing_frame, baselines_path + \".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import data\n",
    "from visualize import visualize\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    # Create embedding for first languages\n",
    "    emb_matrix = data.create_embedding(lang_pair[0], [tsv_cognates_path_train + \".tsv\", tsv_cognates_path_valtest + \".tsv\"])\n",
    "    visualize.visualize_encoding(emb_matrix, feature_matrix_phon, lang_pair, config[\"results_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify sound correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import visualize\n",
    "\n",
    "for lang_pair in lang_pairs:\n",
    "    visualize.show_output_substitutions(results_path[lang_pair], subs_st_path[lang_pair], subs_sp_path[lang_pair])\n",
    "    visualize.visualize_weights(context_vectors_path[lang_pair], lang_pair, config[\"input_encoding\"], config[\"output_encoding\"], config[\"results_dir\"], sample=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phylogenetic tree reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering based on word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Cluster based on word prediction distances\n",
    "print(\"WP TREE:\\n\")\n",
    "cluster.cluster_languages(lang_pairs, distances_path, output_path=distances_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree import cluster\n",
    "\n",
    "# Source prediction baseline\n",
    "print(\"\\nSOURCE BASELINE TREE\")\n",
    "cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_source\", distance_col=2)\n",
    "# PMI-based baseline\n",
    "print(\"\\nPMI BASELINE TREE\")\n",
    "cluster.cluster_languages(lang_pairs, baselines_path, output_path=baselines_path + \"_pmi\", distance_col=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cognate detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognatedetection import cd\n",
    "\n",
    "### TODO: this part from previous code should not be executed:\n",
    "# print(\"Filter val/test sets on cognates.\")\n",
    "# Use only cognate pairs for validation and test\n",
    "# val[lang_pair] = val[lang_pair].filter_cognates()\n",
    "# test[lang_pair] = test[lang_pair].filter_cognates()\n",
    "# print(\"Val/test sizes after cognate filtering: \" + str(val[lang_pair].get_size()) + \"|\" + str(test[lang_pair].get_size()))\n",
    "\n",
    "\n",
    "print(\"Performing WP cognate detection using clustering...\")\n",
    "results_table = cd.cognate_detection_cluster(lang_pairs, config[\"results_dir\"], options, use_distance=\"prediction\")\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phylogenetic word prediction (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import utility\n",
    "from prediction import prediction\n",
    "\n",
    "\n",
    "# In phylogenetic mode, we created one feature matrix for all languages\n",
    "for lang_pair in lang_pairs:\n",
    "    conversion_key[lang_pair] = conversion_key_general\n",
    "\n",
    "voc_size = voc_size_general\n",
    "# For phylogenetic word prediction, create one feature matrix for all languages\n",
    "print(\"Create feature matrix for all language pairs.\")\n",
    "used_tokens = [[], []]\n",
    "tokens_set = [[], []]\n",
    "for lang_pair in lang_pairs:\n",
    "    # For phylogenetic word prediction, create one feature matrix for all languages\n",
    "    features_lp[lang_pair], max_len[lang_pair[0]], max_len[lang_pair[1]], _, _ = data.get_corpus_info([tsv_cognates_path_train + \".tsv\", tsv_cognates_path_valtest + \".tsv\"], lang_pair=lang_pair, input_encoding=config[\"input_encoding\"], output_encoding=config[\"output_encoding\"], feature_matrix_phon=feature_matrix_phon)\n",
    "    used_tokens[0] += list(features_lp[lang_pair][0].index)\n",
    "    used_tokens[1] += list(features_lp[lang_pair][1].index)\n",
    "\n",
    "tokens_set[0] = list(set(used_tokens[0]))\n",
    "tokens_set[1] = list(set(used_tokens[1]))\n",
    "if config[\"input_encoding\"] == \"character\":\n",
    "    features[0] = data.create_one_hot_matrix(tokens_set[0])\n",
    "elif config[\"input_encoding\"] == \"phonetic\":\n",
    "    features[0] = feature_matrix_phon.loc[tokens_set[0]]\n",
    "else:\n",
    "    print(\"Embedding encoding not possible in phylogenetic tree prediction.\")\n",
    "    return\n",
    "# Output encoding is always character\n",
    "features[1] = data.create_one_hot_matrix(tokens_set[1])\n",
    "voc_size_general[0] = features[0].shape[1]\n",
    "voc_size_general[1] = features[1].shape[1]\n",
    "conversion_key_general = data.create_conversion_key(features)\n",
    "plot_path_phyl = utility.create_path(config[\"results_dir\"], options, prefix=\"plot_\")\n",
    "\n",
    "config[\"export_weights\"] = False  # Turn off export of weights\n",
    "tree_string = \"((nld,deu),eng)\"  # unused at the moment\n",
    "if len(config[\"languages\"]) >= 3:\n",
    "    results_path_proto = utility.create_path(config[\"results_dir\"], options, prefix=\"proto_\")  # lang-pair independent path\n",
    "    prediction.word_prediction_phyl(config[\"languages\"], lang_pairs, tree_string, max_len, train, val, test, conversion_key_general, voc_size, results_path, results_path_proto, distances_path + \".txt\", context_vectors_path, plot_path_phyl, config[\"output_encoding\"], config)\n",
    "else:\n",
    "    print(\"Please supply 3 languages, the first 2 being more closely related than the last.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
